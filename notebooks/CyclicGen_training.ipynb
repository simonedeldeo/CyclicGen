{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di 07. CyclicGen - training s1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh_8ShQ78yFX",
        "colab_type": "code",
        "outputId": "40904fab-88d5-4cbb-ad89-4945912a85d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4iGc0tW-nFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# dhash used for shot separation / frame skipping\n",
        "!pip install dhash\n",
        "\n",
        "# module 'scipy.misc' has no attribute 'imread' => downgrade scipy\n",
        "!pip3 install scipy==1.1.0\n",
        "\n",
        "# ValueError: Object arrays cannot be loaded when allow_pickle=False\n",
        "# This version of numpy has the default value of allow_pickle as True.\n",
        "!pip install numpy==1.16.1\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7zzgU0M71Vr",
        "colab_type": "code",
        "outputId": "5d8e3ae2-cdaa-45f1-ec98-ab9d0a2f71f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/simonedeldeo/CyclicGen.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CyclicGen'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 147 (delta 5), reused 0 (delta 0), pack-reused 133\u001b[K\n",
            "Receiving objects: 100% (147/147), 501.39 KiB | 1.42 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXU7ycJFYBGm",
        "colab_type": "code",
        "outputId": "84b6545e-7cd7-40d0-a04e-0b891d970638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd CyclicGen/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CyclicGen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQqnxyek0IEL",
        "colab_type": "code",
        "outputId": "6ffbd88e-0281-485f-cf77-e50416cb0596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# creaz dataset\n",
        "\n",
        "!rm -rf sep_eps\n",
        "!mkdir data_list\n",
        "\n",
        "import cv2\n",
        "import dhash\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def get_num_bits_different(hash1, hash2):\n",
        "    return bin(hash1 ^ hash2).count('1')\n",
        "\n",
        "vids = ['at_s01e01.mp4', 'at_s01e02.mp4', 'at_s01e03.mp4', 'at_s01e04.mp4', 'sao_e01.mkv', 'Akira.mkv']\n",
        "\n",
        "for vid_path in vids:\n",
        "\n",
        "  cap = cv2.VideoCapture('/content/drive/My Drive/__progetto int sys/data/' + vid_path)\n",
        "  vid_name = vid_path.split('.')[0]\n",
        "  totalFrames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "  if totalFrames > 70000:\n",
        "    totalFrames = 70000\n",
        "  main_dir = 'sep_eps/' + vid_name\n",
        "  if os.path.exists(main_dir):\n",
        "      shutil.rmtree(main_dir)\n",
        "  os.makedirs(main_dir)\n",
        "\n",
        "  # 0 - 2: frame ripetuto => non lo salvo\n",
        "  # 0 - 9 per Akira\n",
        "  thresh_rip = 2\n",
        "  if vid_name == 'Akira':\n",
        "    thresh_rip = 9\n",
        "\n",
        "  id_scena = 1\n",
        "  ret, prec = cap.read()\n",
        "  path = main_dir + '/' + str(id_scena).zfill(3)\n",
        "  if not os.path.exists(path):\n",
        "      os.mkdir(path)\n",
        "  cv2.imwrite(path + '/0.jpg', prec)\n",
        "  x = int(prec.shape[0] / 5)\n",
        "  y = int(prec.shape[1] / 5)\n",
        "  for i in range(1, int(totalFrames)):\n",
        "      try:\n",
        "          ret, frame = cap.read()\n",
        "          prec_res = cv2.resize(prec, (y, x))\n",
        "          frame_res = cv2.resize(frame, (y, x))\n",
        "          h1 = dhash.dhash_int(Image.fromarray(prec_res), size=16)\n",
        "          h2 = dhash.dhash_int(Image.fromarray(frame_res), size=16)\n",
        "\n",
        "          bit_diff = get_num_bits_different(h1, h2)\n",
        "\n",
        "          if bit_diff > thresh_rip:\n",
        "              # threshold trovato sperimentalmente per sep shot = 213\n",
        "              # usando 205, i falsi positivi aumentavano solo di 5\n",
        "              # meglio qualche falso pos in piu' che un falso neg\n",
        "              if bit_diff > 204:\n",
        "                  id_scena += 1\n",
        "              path = main_dir + '/' + str(id_scena).zfill(3)\n",
        "              if not os.path.exists(path):\n",
        "                  os.mkdir(path)\n",
        "              frame = cv2.resize(frame, (1280, 720))\n",
        "              cv2.imwrite(path + '/' + str(i) + '.jpg', frame)\n",
        "\n",
        "          prec = frame\n",
        "\n",
        "      except:\n",
        "          print('frame finale: ', i)\n",
        "  print(vid_name)\n",
        "\n",
        "  #########################################################\n",
        "\n",
        "  ep_path = 'sep_eps/' + vid_name\n",
        "  scenes = os.listdir(ep_path)\n",
        "  scenes.sort()\n",
        "\n",
        "  da_canc = []\n",
        "  for s in scenes:\n",
        "      folder = os.path.join(ep_path, s)\n",
        "      frames_number = os.listdir(folder)\n",
        "      if len(frames_number)<3:\n",
        "          da_canc.append(folder)\n",
        "\n",
        "  for f in da_canc:\n",
        "      shutil.rmtree(f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "at_s01e01\n",
            "at_s01e02\n",
            "at_s01e03\n",
            "at_s01e04\n",
            "frame finale:  68573\n",
            "frame finale:  68574\n",
            "sao_e01\n",
            "Akira\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTh7frRdMjIS",
        "colab_type": "code",
        "outputId": "f0b5d083-44ca-4554-d906-ebb3d7ecba4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!du -hs sep_eps"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23G\tsep_eps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P_IRwr0LWRv",
        "colab_type": "code",
        "outputId": "320473ae-2dbd-41e7-940e-2238e2cda36b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "dirs = os.listdir('sep_eps')\n",
        "triplets = []\n",
        "for main_dir in dirs:\n",
        "  lista_dir = os.listdir(os.path.join('sep_eps', main_dir))\n",
        "  lista_dir.sort()\n",
        "  for d in lista_dir:\n",
        "    dir_path = os.path.join('sep_eps', main_dir, d)\n",
        "    toAppend = os.listdir(dir_path)\n",
        "    if len(toAppend)>2:\n",
        "      files = []\n",
        "      for f in toAppend:\n",
        "        files.append(os.path.join(dir_path, f))\n",
        "      files.sort()\n",
        "      for i in range(len(files)-2):\n",
        "        triplets.append([files[i], files[i+1], files[i+2]])\n",
        "      \n",
        "print(len(triplets))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8W8pum-LpJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for t in triplets:\n",
        "  for i in range(1, 4):\n",
        "    path = 'data_list/ucf101_train_files_frame' + str(i) + '.txt'\n",
        "    with open(path, 'a') as fp:\n",
        "      fp.write(t[i-1] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODwmw1B88XPA",
        "colab_type": "code",
        "outputId": "a5fe7a68-c564-4105-c55e-a98017053209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!cp '/content/drive/My Drive/__progetto int sys/ckps/CyclicGen/ckpt_and_hed_model.zip' '/content/CyclicGen'\n",
        "!unzip ckpt_and_hed_model.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ckpt_and_hed_model.zip\n",
            "   creating: ckpt/\n",
            "   creating: ckpt/CyclicGen/\n",
            "  inflating: ckpt/CyclicGen/model.meta  \n",
            "  inflating: ckpt/CyclicGen/model.index  \n",
            "  inflating: ckpt/CyclicGen/model.data-00000-of-00001  \n",
            "   creating: ckpt/CyclicGen_large/\n",
            "  inflating: ckpt/CyclicGen_large/model.meta  \n",
            "  inflating: ckpt/CyclicGen_large/model.index  \n",
            "  inflating: ckpt/CyclicGen_large/model.data-00000-of-00001  \n",
            "   creating: hed_model/\n",
            "  inflating: hed_model/new-model.ckpt.meta  \n",
            "  inflating: hed_model/new-model.ckpt.data-00000-of-00001  \n",
            "  inflating: hed_model/new-model.ckpt.index  \n",
            "  inflating: vgg16.npy               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbu__6R78M6y",
        "colab_type": "code",
        "outputId": "04f20e34-e32a-4fd0-baf4-b1f93b449f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!date\n",
        "!python3 CyclicGen_train_stage1.py --subset=train --train_dir='/content/drive/My Drive/__progetto int sys/ckps/CyclicGen/_new/prv_1ep720_s1' --pretrained_model_checkpoint_path=./ckpt/CyclicGen/model --max_steps=20826 | tee '/content/drive/My Drive/__progetto int sys/ckps/CyclicGen/_new/prv_1ep720_s1.txt'\n",
        "!date"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb  5 12:16:22 UTC 2020\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:59: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.shuffle_and_repeat(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/shuffle_ops.py:54: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.read_file is deprecated. Please use tf.io.read_file instead.\n",
            "\n",
            "2020-02-05 12:16:34.647554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-02-05 12:16:34.689283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-05 12:16:34.689964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-02-05 12:16:34.715094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-02-05 12:16:34.931837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-02-05 12:16:35.062631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-02-05 12:16:35.083340: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-02-05 12:16:35.354318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-02-05 12:16:35.373102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-02-05 12:16:35.876993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-02-05 12:16:35.877238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-05 12:16:35.878383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-05 12:16:35.879251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:86: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /content/CyclicGen/vgg16.py:45: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/CyclicGen/vgg16.py:178: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/CyclicGen/vgg16.py:123: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/CyclicGen/vgg16.py:102: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/CyclicGen/vgg16.py:87: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/CyclicGen/CyclicGen_model.py:81: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/CyclicGen/utils/geo_layer_utils.py:37: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/CyclicGen/utils/loss_utils.py:21: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:109: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:122: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:128: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:128: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:129: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:130: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:138: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:138: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Please use tf.global_variables instead.\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:141: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:145: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-02-05 12:16:40.889794: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2020-02-05 12:16:40.901470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000179999 Hz\n",
            "2020-02-05 12:16:40.901865: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2feb2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-02-05 12:16:40.901899: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-02-05 12:16:41.031234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-05 12:16:41.031894: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2feb100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-02-05 12:16:41.031918: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-02-05 12:16:41.033468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-05 12:16:41.033969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-02-05 12:16:41.034056: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-02-05 12:16:41.034077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-02-05 12:16:41.034088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-02-05 12:16:41.034099: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-02-05 12:16:41.034113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-02-05 12:16:41.034124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-02-05 12:16:41.034135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-02-05 12:16:41.034187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-05 12:16:41.034762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-05 12:16:41.035288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-02-05 12:16:41.038419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-02-05 12:16:41.039826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-02-05 12:16:41.039857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-02-05 12:16:41.039868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-02-05 12:16:41.040879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-05 12:16:41.041506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-05 12:16:41.042077: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-02-05 12:16:41.042121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From CyclicGen_train_stage1.py:162: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "2020-02-05 12:16:53.908226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-02-05 12:16:58.642881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "all layers:\n",
            "hed/side_1/Variable:0\n",
            "hed/side_1/Variable_1:0\n",
            "hed/side_1/Variable_2:0\n",
            "hed/side_2/Variable:0\n",
            "hed/side_2/Variable_1:0\n",
            "hed/side_2/Variable_2:0\n",
            "hed/side_3/Variable:0\n",
            "hed/side_3/Variable_1:0\n",
            "hed/side_3/Variable_2:0\n",
            "hed/side_4/Variable:0\n",
            "hed/side_4/Variable_1:0\n",
            "hed/side_4/Variable_2:0\n",
            "hed/side_5/Variable:0\n",
            "hed/side_5/Variable_1:0\n",
            "hed/side_5/Variable_2:0\n",
            "hed/Variable:0\n",
            "Cycle_DVF/conv1/weights:0\n",
            "Cycle_DVF/conv1/BatchNorm/beta:0\n",
            "Cycle_DVF/conv2/weights:0\n",
            "Cycle_DVF/conv2/BatchNorm/beta:0\n",
            "Cycle_DVF/conv3/weights:0\n",
            "Cycle_DVF/conv3/BatchNorm/beta:0\n",
            "Cycle_DVF/conv4/weights:0\n",
            "Cycle_DVF/conv4/BatchNorm/beta:0\n",
            "Cycle_DVF/conv5/weights:0\n",
            "Cycle_DVF/conv5/BatchNorm/beta:0\n",
            "Cycle_DVF/conv6/weights:0\n",
            "Cycle_DVF/conv6/BatchNorm/beta:0\n",
            "Cycle_DVF/conv7/weights:0\n",
            "Cycle_DVF/conv7/BatchNorm/beta:0\n",
            "Cycle_DVF/conv8/weights:0\n",
            "Cycle_DVF/conv8/biases:0\n",
            "optimize layers:\n",
            "Cycle_DVF/conv1/weights:0\n",
            "Cycle_DVF/conv1/BatchNorm/beta:0\n",
            "Cycle_DVF/conv2/weights:0\n",
            "Cycle_DVF/conv2/BatchNorm/beta:0\n",
            "Cycle_DVF/conv3/weights:0\n",
            "Cycle_DVF/conv3/BatchNorm/beta:0\n",
            "Cycle_DVF/conv4/weights:0\n",
            "Cycle_DVF/conv4/BatchNorm/beta:0\n",
            "Cycle_DVF/conv5/weights:0\n",
            "Cycle_DVF/conv5/BatchNorm/beta:0\n",
            "Cycle_DVF/conv6/weights:0\n",
            "Cycle_DVF/conv6/BatchNorm/beta:0\n",
            "Cycle_DVF/conv7/weights:0\n",
            "Cycle_DVF/conv7/BatchNorm/beta:0\n",
            "Cycle_DVF/conv8/weights:0\n",
            "Cycle_DVF/conv8/biases:0\n",
            "2020-02-05 12:16:41.552371: Pre-trained model restored from ./ckpt/CyclicGen/model\n",
            "Epoch Number: 0\n",
            "Loss at step 0: 0.044918\n",
            "Loss at step 10: 0.017695\n",
            "Loss at step 20: 0.105821\n",
            "Loss at step 30: 0.104564\n",
            "Loss at step 40: 0.068633\n",
            "Loss at step 50: 0.010928\n",
            "Loss at step 60: 0.018222\n",
            "Loss at step 70: 0.015954\n",
            "Loss at step 80: 0.028353\n",
            "Loss at step 90: 0.046663\n",
            "Loss at step 100: 0.020393\n",
            "Loss at step 110: 0.030658\n",
            "Loss at step 120: 0.044164\n",
            "Loss at step 130: 0.022453\n",
            "Loss at step 140: 0.029969\n",
            "Loss at step 150: 0.020704\n",
            "Loss at step 160: 0.013379\n",
            "Loss at step 170: 0.017154\n",
            "Loss at step 180: 0.149442\n",
            "Loss at step 190: 0.049607\n",
            "Loss at step 200: 0.023082\n",
            "Loss at step 210: 0.089504\n",
            "Loss at step 220: 0.032447\n",
            "Loss at step 230: 0.023726\n",
            "Loss at step 240: 0.081673\n",
            "Loss at step 250: 0.039898\n",
            "Loss at step 260: 0.043056\n",
            "Loss at step 270: 0.049305\n",
            "Loss at step 280: 0.029974\n",
            "Loss at step 290: 0.028784\n",
            "Loss at step 300: 0.022601\n",
            "Loss at step 310: 0.083720\n",
            "Loss at step 320: 0.043550\n",
            "Loss at step 330: 0.018807\n",
            "Loss at step 340: 0.021835\n",
            "Loss at step 350: 0.049430\n",
            "Loss at step 360: 0.015929\n",
            "Loss at step 370: 0.017814\n",
            "Loss at step 380: 0.039424\n",
            "Loss at step 390: 0.025822\n",
            "Loss at step 400: 0.056081\n",
            "Loss at step 410: 0.027687\n",
            "Loss at step 420: 0.040575\n",
            "Loss at step 430: 0.030399\n",
            "Loss at step 440: 0.026679\n",
            "Loss at step 450: 0.016493\n",
            "Loss at step 460: 0.010307\n",
            "Loss at step 470: 0.024641\n",
            "Loss at step 480: 0.028191\n",
            "Loss at step 490: 0.044568\n",
            "Loss at step 500: 0.053365\n",
            "Loss at step 510: 0.022716\n",
            "Loss at step 520: 0.088222\n",
            "Loss at step 530: 0.019630\n",
            "Loss at step 540: 0.047408\n",
            "Loss at step 550: 0.044127\n",
            "Loss at step 560: 0.048005\n",
            "Loss at step 570: 0.046371\n",
            "Loss at step 580: 0.043476\n",
            "Loss at step 590: 0.044705\n",
            "Loss at step 600: 0.020396\n",
            "Loss at step 610: 0.015335\n",
            "Loss at step 620: 0.035337\n",
            "Loss at step 630: 0.048493\n",
            "Loss at step 640: 0.049633\n",
            "Loss at step 650: 0.026182\n",
            "Loss at step 660: 0.046484\n",
            "Loss at step 670: 0.028014\n",
            "Loss at step 680: 0.039789\n",
            "Loss at step 690: 0.021891\n",
            "Loss at step 700: 0.023731\n",
            "Loss at step 710: 0.030235\n",
            "Loss at step 720: 0.019826\n",
            "Loss at step 730: 0.023198\n",
            "Loss at step 740: 0.071001\n",
            "Loss at step 750: 0.038624\n",
            "Loss at step 760: 0.025668\n",
            "Loss at step 770: 0.049003\n",
            "Loss at step 780: 0.046756\n",
            "Loss at step 790: 0.023878\n",
            "Loss at step 800: 0.031058\n",
            "Loss at step 810: 0.031533\n",
            "Loss at step 820: 0.023821\n",
            "Loss at step 830: 0.066446\n",
            "Loss at step 840: 0.009824\n",
            "Loss at step 850: 0.052970\n",
            "Loss at step 860: 0.103886\n",
            "Loss at step 870: 0.016734\n",
            "Loss at step 880: 0.011472\n",
            "Loss at step 890: 0.029731\n",
            "Loss at step 900: 0.065504\n",
            "Loss at step 910: 0.038888\n",
            "Loss at step 920: 0.018499\n",
            "Loss at step 930: 0.009083\n",
            "Loss at step 940: 0.019862\n",
            "Loss at step 950: 0.029366\n",
            "Loss at step 960: 0.069577\n",
            "Loss at step 970: 0.037838\n",
            "Loss at step 980: 0.022244\n",
            "Loss at step 990: 0.020304\n",
            "Loss at step 1000: 0.018232\n",
            "Loss at step 1010: 0.015428\n",
            "Loss at step 1020: 0.022810\n",
            "Loss at step 1030: 0.041004\n",
            "Loss at step 1040: 0.071468\n",
            "Loss at step 1050: 0.058568\n",
            "Loss at step 1060: 0.036187\n",
            "Loss at step 1070: 0.007349\n",
            "Loss at step 1080: 0.024265\n",
            "Loss at step 1090: 0.024902\n",
            "Loss at step 1100: 0.010673\n",
            "Loss at step 1110: 0.014869\n",
            "Loss at step 1120: 0.077013\n",
            "Loss at step 1130: 0.083064\n",
            "Loss at step 1140: 0.034085\n",
            "Loss at step 1150: 0.037125\n",
            "Loss at step 1160: 0.042389\n",
            "Loss at step 1170: 0.025400\n",
            "Loss at step 1180: 0.024750\n",
            "Loss at step 1190: 0.014576\n",
            "Loss at step 1200: 0.026948\n",
            "Loss at step 1210: 0.022049\n",
            "Loss at step 1220: 0.027475\n",
            "Loss at step 1230: 0.029141\n",
            "Loss at step 1240: 0.022208\n",
            "Loss at step 1250: 0.058167\n",
            "Loss at step 1260: 0.099060\n",
            "Loss at step 1270: 0.011997\n",
            "Loss at step 1280: 0.015882\n",
            "Loss at step 1290: 0.044314\n",
            "Loss at step 1300: 0.017461\n",
            "Loss at step 1310: 0.056108\n",
            "Loss at step 1320: 0.050768\n",
            "Loss at step 1330: 0.029563\n",
            "Loss at step 1340: 0.031376\n",
            "Loss at step 1350: 0.039874\n",
            "Loss at step 1360: 0.093313\n",
            "Loss at step 1370: 0.041806\n",
            "Loss at step 1380: 0.059449\n",
            "Loss at step 1390: 0.032870\n",
            "Loss at step 1400: 0.017368\n",
            "Loss at step 1410: 0.035601\n",
            "Loss at step 1420: 0.053365\n",
            "Loss at step 1430: 0.011454\n",
            "Loss at step 1440: 0.039475\n",
            "Loss at step 1450: 0.028831\n",
            "Loss at step 1460: 0.032736\n",
            "Loss at step 1470: 0.048606\n",
            "Loss at step 1480: 0.020173\n",
            "Loss at step 1490: 0.044851\n",
            "Loss at step 1500: 0.046836\n",
            "Loss at step 1510: 0.046097\n",
            "Loss at step 1520: 0.067045\n",
            "Loss at step 1530: 0.016923\n",
            "Loss at step 1540: 0.043327\n",
            "Loss at step 1550: 0.024992\n",
            "Loss at step 1560: 0.024231\n",
            "Loss at step 1570: 0.081635\n",
            "Loss at step 1580: 0.022049\n",
            "Loss at step 1590: 0.024728\n",
            "Loss at step 1600: 0.049905\n",
            "Loss at step 1610: 0.096744\n",
            "Loss at step 1620: 0.032336\n",
            "Loss at step 1630: 0.038550\n",
            "Loss at step 1640: 0.023138\n",
            "Loss at step 1650: 0.012877\n",
            "Loss at step 1660: 0.041786\n",
            "Loss at step 1670: 0.047617\n",
            "Loss at step 1680: 0.040466\n",
            "Loss at step 1690: 0.041562\n",
            "Loss at step 1700: 0.045423\n",
            "Loss at step 1710: 0.013757\n",
            "Loss at step 1720: 0.064483\n",
            "Loss at step 1730: 0.035490\n",
            "Loss at step 1740: 0.026107\n",
            "Loss at step 1750: 0.042896\n",
            "Loss at step 1760: 0.023418\n",
            "Loss at step 1770: 0.031013\n",
            "Loss at step 1780: 0.026653\n",
            "Loss at step 1790: 0.056230\n",
            "Loss at step 1800: 0.028681\n",
            "Loss at step 1810: 0.016992\n",
            "Loss at step 1820: 0.062286\n",
            "Loss at step 1830: 0.008540\n",
            "Loss at step 1840: 0.018601\n",
            "Loss at step 1850: 0.020390\n",
            "Loss at step 1860: 0.044625\n",
            "Loss at step 1870: 0.034953\n",
            "Loss at step 1880: 0.021989\n",
            "Loss at step 1890: 0.031967\n",
            "Loss at step 1900: 0.054209\n",
            "Loss at step 1910: 0.014100\n",
            "Loss at step 1920: 0.036371\n",
            "Loss at step 1930: 0.034895\n",
            "Loss at step 1940: 0.021529\n",
            "Loss at step 1950: 0.031265\n",
            "Loss at step 1960: 0.036517\n",
            "Loss at step 1970: 0.015467\n",
            "Loss at step 1980: 0.022416\n",
            "Loss at step 1990: 0.019733\n",
            "Loss at step 2000: 0.025855\n",
            "Loss at step 2010: 0.040461\n",
            "Loss at step 2020: 0.050085\n",
            "Loss at step 2030: 0.023764\n",
            "Loss at step 2040: 0.017644\n",
            "Loss at step 2050: 0.031691\n",
            "Loss at step 2060: 0.014140\n",
            "Loss at step 2070: 0.054173\n",
            "Loss at step 2080: 0.052095\n",
            "Loss at step 2090: 0.024779\n",
            "Loss at step 2100: 0.026861\n",
            "Loss at step 2110: 0.013435\n",
            "Loss at step 2120: 0.038351\n",
            "Loss at step 2130: 0.024608\n",
            "Loss at step 2140: 0.016687\n",
            "Loss at step 2150: 0.008950\n",
            "Loss at step 2160: 0.015805\n",
            "Loss at step 2170: 0.045415\n",
            "Loss at step 2180: 0.029275\n",
            "Loss at step 2190: 0.024086\n",
            "Loss at step 2200: 0.018813\n",
            "Loss at step 2210: 0.063593\n",
            "Loss at step 2220: 0.022733\n",
            "Loss at step 2230: 0.025032\n",
            "Loss at step 2240: 0.114051\n",
            "Loss at step 2250: 0.031510\n",
            "Loss at step 2260: 0.082144\n",
            "Loss at step 2270: 0.032854\n",
            "Loss at step 2280: 0.048175\n",
            "Loss at step 2290: 0.014395\n",
            "Loss at step 2300: 0.053593\n",
            "Loss at step 2310: 0.026459\n",
            "Loss at step 2320: 0.037900\n",
            "Loss at step 2330: 0.059380\n",
            "Loss at step 2340: 0.036434\n",
            "Loss at step 2350: 0.030184\n",
            "Loss at step 2360: 0.048820\n",
            "Loss at step 2370: 0.033865\n",
            "Loss at step 2380: 0.040827\n",
            "Loss at step 2390: 0.036640\n",
            "Loss at step 2400: 0.026681\n",
            "Loss at step 2410: 0.038587\n",
            "Loss at step 2420: 0.014387\n",
            "Loss at step 2430: 0.032374\n",
            "Loss at step 2440: 0.036440\n",
            "Loss at step 2450: 0.0377202020-02-05 12:43:37.695430: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23777280 exceeds 10% of system memory.\n",
            "2020-02-05 12:43:37.729139: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23777280 exceeds 10% of system memory.\n",
            "2020-02-05 12:43:37.744603: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23777280 exceeds 10% of system memory.\n",
            "\n",
            "Loss at step 2460: 0.049046\n",
            "Loss at step 2470: 0.013962\n",
            "Loss at step 2480: 0.030641\n",
            "Loss at step 2490: 0.022578\n",
            "Loss at step 2500: 0.031287\n",
            "Loss at step 2510: 0.029660\n",
            "Loss at step 2520: 0.022597\n",
            "Loss at step 2530: 0.039048\n",
            "Loss at step 2540: 0.027181\n",
            "Loss at step 2550: 0.018221\n",
            "Loss at step 2560: 0.018157\n",
            "Loss at step 2570: 0.019480\n",
            "Loss at step 2580: 0.030180\n",
            "Loss at step 2590: 0.022478\n",
            "Loss at step 2600: 0.042073\n",
            "Loss at step 2610: 0.010768\n",
            "Loss at step 2620: 0.018520\n",
            "Loss at step 2630: 0.047382\n",
            "Loss at step 2640: 0.028580\n",
            "Loss at step 2650: 0.047900\n",
            "Loss at step 2660: 0.066359\n",
            "Loss at step 2670: 0.057321\n",
            "Loss at step 2680: 0.020097\n",
            "Loss at step 2690: 0.029676\n",
            "Loss at step 2700: 0.064509\n",
            "Loss at step 2710: 0.023139\n",
            "Loss at step 2720: 0.045493\n",
            "Loss at step 2730: 0.041396\n",
            "Loss at step 2740: 0.016720\n",
            "Loss at step 2750: 0.069833\n",
            "Loss at step 2760: 0.060241\n",
            "Loss at step 2770: 0.024910\n",
            "Loss at step 2780: 0.072316\n",
            "Loss at step 2790: 0.112247\n",
            "Loss at step 2800: 0.035187\n",
            "Loss at step 2810: 0.056583\n",
            "Loss at step 2820: 0.022525\n",
            "Loss at step 2830: 0.023935\n",
            "Loss at step 2840: 0.052780\n",
            "Loss at step 2850: 0.023992\n",
            "Loss at step 2860: 0.021332\n",
            "Loss at step 2870: 0.064605\n",
            "Loss at step 2880: 0.031736\n",
            "Loss at step 2890: 0.057133\n",
            "Loss at step 2900: 0.018780\n",
            "Loss at step 2910: 0.012683\n",
            "Loss at step 2920: 0.017113\n",
            "Loss at step 2930: 0.024462\n",
            "Loss at step 2940: 0.019771\n",
            "Loss at step 2950: 0.018026\n",
            "Loss at step 2960: 0.028211\n",
            "Loss at step 2970: 0.045663\n",
            "Loss at step 2980: 0.032718\n",
            "Loss at step 2990: 0.023789\n",
            "Loss at step 3000: 0.035006\n",
            "Loss at step 3010: 0.008409\n",
            "Loss at step 3020: 0.010525\n",
            "Loss at step 3030: 0.011793\n",
            "Loss at step 3040: 0.027208\n",
            "Loss at step 3050: 0.010474\n",
            "Loss at step 3060: 0.017014\n",
            "Loss at step 3070: 0.032479\n",
            "Loss at step 3080: 0.030776\n",
            "Loss at step 3090: 0.021110\n",
            "Loss at step 3100: 0.031227\n",
            "Loss at step 3110: 0.049875\n",
            "Loss at step 3120: 0.034593\n",
            "Loss at step 3130: 0.049305\n",
            "Loss at step 3140: 0.008775\n",
            "Loss at step 3150: 0.051615\n",
            "Loss at step 3160: 0.036089\n",
            "Loss at step 3170: 0.031949\n",
            "Loss at step 3180: 0.024248\n",
            "Loss at step 3190: 0.015057\n",
            "Loss at step 3200: 0.021247\n",
            "Loss at step 3210: 0.054093\n",
            "Loss at step 3220: 0.024037\n",
            "Loss at step 3230: 0.037700\n",
            "Loss at step 3240: 0.049278\n",
            "Loss at step 3250: 0.023312\n",
            "Loss at step 3260: 0.034237\n",
            "Loss at step 3270: 0.062373\n",
            "Loss at step 3280: 0.019587\n",
            "Loss at step 3290: 0.043188\n",
            "Loss at step 3300: 0.051349\n",
            "Loss at step 3310: 0.061454\n",
            "Loss at step 3320: 0.048177\n",
            "Loss at step 3330: 0.034614\n",
            "Loss at step 3340: 0.029868\n",
            "Loss at step 3350: 0.008731\n",
            "Loss at step 3360: 0.058534\n",
            "Loss at step 3370: 0.031928\n",
            "Loss at step 3380: 0.032551\n",
            "Loss at step 3390: 0.054504\n",
            "Loss at step 3400: 0.067055\n",
            "Loss at step 3410: 0.036112\n",
            "Loss at step 3420: 0.012579\n",
            "Loss at step 3430: 0.036198\n",
            "Loss at step 3440: 0.073815\n",
            "Loss at step 3450: 0.042789\n",
            "Loss at step 3460: 0.023860\n",
            "Loss at step 3470: 0.011150\n",
            "Loss at step 3480: 0.025682\n",
            "Loss at step 3490: 0.015296\n",
            "Loss at step 3500: 0.071772\n",
            "Loss at step 3510: 0.027524\n",
            "Loss at step 3520: 0.063124\n",
            "Loss at step 3530: 0.079615\n",
            "Loss at step 3540: 0.015872\n",
            "Loss at step 3550: 0.041126\n",
            "Loss at step 3560: 0.042371\n",
            "Loss at step 3570: 0.021936\n",
            "Loss at step 3580: 0.030275\n",
            "Loss at step 3590: 0.009077\n",
            "Loss at step 3600: 0.051452\n",
            "Loss at step 3610: 0.073511\n",
            "Loss at step 3620: 0.025089\n",
            "Loss at step 3630: 0.034451\n",
            "Loss at step 3640: 0.052742\n",
            "Loss at step 3650: 0.041159\n",
            "Loss at step 3660: 0.018934\n",
            "Loss at step 3670: 0.042425\n",
            "Loss at step 3680: 0.012428\n",
            "Loss at step 3690: 0.044848\n",
            "Loss at step 3700: 0.078446\n",
            "Loss at step 3710: 0.008342\n",
            "Loss at step 3720: 0.018052\n",
            "Loss at step 3730: 0.024605\n",
            "Loss at step 3740: 0.031628\n",
            "Loss at step 3750: 0.041904\n",
            "Loss at step 3760: 0.033182\n",
            "Loss at step 3770: 0.006232\n",
            "Loss at step 3780: 0.022663\n",
            "Loss at step 3790: 0.022893\n",
            "Loss at step 3800: 0.008709\n",
            "Loss at step 3810: 0.036553\n",
            "Loss at step 3820: 0.026217\n",
            "Loss at step 3830: 0.110638\n",
            "Loss at step 3840: 0.018019\n",
            "Loss at step 3850: 0.039236\n",
            "Loss at step 3860: 0.036988\n",
            "Loss at step 3870: 0.023955\n",
            "Loss at step 3880: 0.031479\n",
            "Loss at step 3890: 0.036902\n",
            "Loss at step 3900: 0.039206\n",
            "Loss at step 3910: 0.031206\n",
            "Loss at step 3920: 0.019090\n",
            "Loss at step 3930: 0.019503\n",
            "Loss at step 3940: 0.027320\n",
            "Loss at step 3950: 0.023910\n",
            "Loss at step 3960: 0.039947\n",
            "Loss at step 3970: 0.043376\n",
            "Loss at step 3980: 0.014409\n",
            "Loss at step 3990: 0.029749\n",
            "Loss at step 4000: 0.012426\n",
            "Loss at step 4010: 0.034059\n",
            "Loss at step 4020: 0.037174\n",
            "Loss at step 4030: 0.049734\n",
            "Loss at step 4040: 0.028870\n",
            "Loss at step 4050: 0.028954\n",
            "Loss at step 4060: 0.027139\n",
            "Loss at step 4070: 0.029936\n",
            "Loss at step 4080: 0.035669\n",
            "Loss at step 4090: 0.050696\n",
            "Loss at step 4100: 0.040997\n",
            "Loss at step 4110: 0.035908\n",
            "Loss at step 4120: 0.036661\n",
            "Loss at step 4130: 0.029853\n",
            "Loss at step 4140: 0.048186\n",
            "Loss at step 4150: 0.044132\n",
            "Loss at step 4160: 0.013235\n",
            "Loss at step 4170: 0.013957\n",
            "Loss at step 4180: 0.044579\n",
            "Loss at step 4190: 0.031504\n",
            "Loss at step 4200: 0.020186\n",
            "Loss at step 4210: 0.027456\n",
            "Loss at step 4220: 0.024371\n",
            "Loss at step 4230: 0.058024\n",
            "Loss at step 4240: 0.065635\n",
            "Loss at step 4250: 0.058051\n",
            "Loss at step 4260: 0.022184\n",
            "Loss at step 4270: 0.087037\n",
            "Loss at step 4280: 0.033577\n",
            "Loss at step 4290: 0.031514\n",
            "Loss at step 4300: 0.027121\n",
            "Loss at step 4310: 0.039329\n",
            "Loss at step 4320: 0.022562\n",
            "Loss at step 4330: 0.014434\n",
            "Loss at step 4340: 0.030341\n",
            "Loss at step 4350: 0.034710\n",
            "Loss at step 4360: 0.007561\n",
            "Loss at step 4370: 0.049871\n",
            "Loss at step 4380: 0.022386\n",
            "Loss at step 4390: 0.053155\n",
            "Loss at step 4400: 0.021483\n",
            "Loss at step 4410: 0.040063\n",
            "Loss at step 4420: 0.019601\n",
            "Loss at step 4430: 0.038495\n",
            "Loss at step 4440: 0.057687\n",
            "Loss at step 4450: 0.018635\n",
            "Loss at step 4460: 0.028512\n",
            "Loss at step 4470: 0.031086\n",
            "Loss at step 4480: 0.034905\n",
            "Loss at step 4490: 0.010819\n",
            "Loss at step 4500: 0.022477\n",
            "Loss at step 4510: 0.016942\n",
            "Loss at step 4520: 0.060153\n",
            "Loss at step 4530: 0.026677\n",
            "Loss at step 4540: 0.036066\n",
            "Loss at step 4550: 0.024034\n",
            "Loss at step 4560: 0.046681\n",
            "Loss at step 4570: 0.018957\n",
            "Loss at step 4580: 0.030129\n",
            "Loss at step 4590: 0.022296\n",
            "Loss at step 4600: 0.020045\n",
            "Loss at step 4610: 0.032928\n",
            "Loss at step 4620: 0.039680\n",
            "Loss at step 4630: 0.018347\n",
            "Loss at step 4640: 0.024881\n",
            "Loss at step 4650: 0.034862\n",
            "Loss at step 4660: 0.013981\n",
            "Loss at step 4670: 0.055962\n",
            "Loss at step 4680: 0.020499\n",
            "Loss at step 4690: 0.024598\n",
            "Loss at step 4700: 0.058509\n",
            "Loss at step 4710: 0.054482\n",
            "Loss at step 4720: 0.042552\n",
            "Loss at step 4730: 0.063255\n",
            "Loss at step 4740: 0.064139\n",
            "Loss at step 4750: 0.009733\n",
            "Loss at step 4760: 0.007012\n",
            "Loss at step 4770: 0.027606\n",
            "Loss at step 4780: 0.010911\n",
            "Loss at step 4790: 0.026547\n",
            "Loss at step 4800: 0.044869\n",
            "Loss at step 4810: 0.030459\n",
            "Loss at step 4820: 0.065195\n",
            "Loss at step 4830: 0.029786\n",
            "Loss at step 4840: 0.017887\n",
            "Loss at step 4850: 0.022745\n",
            "Loss at step 4860: 0.044157\n",
            "Loss at step 4870: 0.037582\n",
            "Loss at step 4880: 0.022585\n",
            "Loss at step 4890: 0.017833\n",
            "Loss at step 4900: 0.036769\n",
            "Loss at step 4910: 0.043361\n",
            "Loss at step 4920: 0.008566\n",
            "Loss at step 4930: 0.021902\n",
            "Loss at step 4940: 0.022455\n",
            "Loss at step 4950: 0.038706\n",
            "Loss at step 4960: 0.036464\n",
            "Loss at step 4970: 0.024261\n",
            "Loss at step 4980: 0.015660\n",
            "Loss at step 4990: 0.051772\n",
            "Loss at step 5000: 0.009703\n",
            "Loss at step 5010: 0.063063\n",
            "Loss at step 5020: 0.021208\n",
            "Loss at step 5030: 0.015116\n",
            "Loss at step 5040: 0.020670\n",
            "Loss at step 5050: 0.067361\n",
            "Loss at step 5060: 0.028275\n",
            "Loss at step 5070: 0.066439\n",
            "Loss at step 5080: 0.006250\n",
            "Loss at step 5090: 0.025680\n",
            "Loss at step 5100: 0.050727\n",
            "Loss at step 5110: 0.015423\n",
            "Loss at step 5120: 0.054230\n",
            "Loss at step 5130: 0.028845\n",
            "Loss at step 5140: 0.020028\n",
            "Loss at step 5150: 0.010839\n",
            "Loss at step 5160: 0.014108\n",
            "Loss at step 5170: 0.025384\n",
            "Loss at step 5180: 0.014767\n",
            "Loss at step 5190: 0.028807\n",
            "Loss at step 5200: 0.024422\n",
            "Loss at step 5210: 0.012367\n",
            "Loss at step 5220: 0.032309\n",
            "Loss at step 5230: 0.031281\n",
            "Loss at step 5240: 0.014828\n",
            "Loss at step 5250: 0.043495\n",
            "Loss at step 5260: 0.043894\n",
            "Loss at step 5270: 0.072857\n",
            "Loss at step 5280: 0.028733\n",
            "Loss at step 5290: 0.012786\n",
            "Loss at step 5300: 0.066112\n",
            "Loss at step 5310: 0.034571\n",
            "Loss at step 5320: 0.034141\n",
            "Loss at step 5330: 0.013880\n",
            "Loss at step 5340: 0.028547\n",
            "Loss at step 5350: 0.048594\n",
            "Loss at step 5360: 0.036267\n",
            "Loss at step 5370: 0.038146\n",
            "Loss at step 5380: 0.051184\n",
            "Loss at step 5390: 0.006823\n",
            "Loss at step 5400: 0.065621\n",
            "Loss at step 5410: 0.046171\n",
            "Loss at step 5420: 0.023853\n",
            "Loss at step 5430: 0.015484\n",
            "Loss at step 5440: 0.049828\n",
            "Loss at step 5450: 0.056934\n",
            "Loss at step 5460: 0.028944\n",
            "Loss at step 5470: 0.044924\n",
            "Loss at step 5480: 0.015309\n",
            "Loss at step 5490: 0.019586\n",
            "Loss at step 5500: 0.048905\n",
            "Loss at step 5510: 0.028290\n",
            "Loss at step 5520: 0.020284\n",
            "Loss at step 5530: 0.005713\n",
            "Loss at step 5540: 0.016475\n",
            "Loss at step 5550: 0.063725\n",
            "Loss at step 5560: 0.084429\n",
            "Loss at step 5570: 0.042082\n",
            "Loss at step 5580: 0.028055\n",
            "Loss at step 5590: 0.020183\n",
            "Loss at step 5600: 0.011689\n",
            "Loss at step 5610: 0.060577\n",
            "Loss at step 5620: 0.012480\n",
            "Loss at step 5630: 0.029931\n",
            "Loss at step 5640: 0.012254\n",
            "Loss at step 5650: 0.015748\n",
            "Loss at step 5660: 0.041594\n",
            "Loss at step 5670: 0.047274\n",
            "Loss at step 5680: 0.131970\n",
            "Loss at step 5690: 0.018418\n",
            "Loss at step 5700: 0.013699\n",
            "Loss at step 5710: 0.033350\n",
            "Loss at step 5720: 0.042691\n",
            "Loss at step 5730: 0.035610\n",
            "Loss at step 5740: 0.027083\n",
            "Loss at step 5750: 0.070929\n",
            "Loss at step 5760: 0.030809\n",
            "Loss at step 5770: 0.037776\n",
            "Loss at step 5780: 0.016022\n",
            "Loss at step 5790: 0.034803\n",
            "Loss at step 5800: 0.023241\n",
            "Loss at step 5810: 0.006583\n",
            "Loss at step 5820: 0.014755\n",
            "Loss at step 5830: 0.047107\n",
            "Loss at step 5840: 0.036522\n",
            "Loss at step 5850: 0.030309\n",
            "Loss at step 5860: 0.036304\n",
            "Loss at step 5870: 0.034906\n",
            "Loss at step 5880: 0.042274\n",
            "Loss at step 5890: 0.018079\n",
            "Loss at step 5900: 0.042486\n",
            "Loss at step 5910: 0.028834\n",
            "Loss at step 5920: 0.014336\n",
            "Loss at step 5930: 0.033222\n",
            "Loss at step 5940: 0.016551\n",
            "Loss at step 5950: 0.015775\n",
            "Loss at step 5960: 0.008660\n",
            "Loss at step 5970: 0.025553\n",
            "Loss at step 5980: 0.065799\n",
            "Loss at step 5990: 0.012575\n",
            "Loss at step 6000: 0.007461\n",
            "Loss at step 6010: 0.013399\n",
            "Loss at step 6020: 0.064544\n",
            "Loss at step 6030: 0.023834\n",
            "Loss at step 6040: 0.024578\n",
            "Loss at step 6050: 0.028755\n",
            "Loss at step 6060: 0.060754\n",
            "Loss at step 6070: 0.010521\n",
            "Loss at step 6080: 0.045486\n",
            "Loss at step 6090: 0.032464\n",
            "Loss at step 6100: 0.029311\n",
            "Loss at step 6110: 0.010555\n",
            "Loss at step 6120: 0.030103\n",
            "Loss at step 6130: 0.097401\n",
            "Loss at step 6140: 0.044496\n",
            "Loss at step 6150: 0.011275\n",
            "Loss at step 6160: 0.007533\n",
            "Loss at step 6170: 0.067568\n",
            "Loss at step 6180: 0.039880\n",
            "Loss at step 6190: 0.023127\n",
            "Loss at step 6200: 0.031817\n",
            "Loss at step 6210: 0.013044\n",
            "Loss at step 6220: 0.025747\n",
            "Loss at step 6230: 0.047611\n",
            "Loss at step 6240: 0.011057\n",
            "Loss at step 6250: 0.014994\n",
            "Loss at step 6260: 0.028993\n",
            "Loss at step 6270: 0.015032\n",
            "Loss at step 6280: 0.053588\n",
            "Loss at step 6290: 0.065521\n",
            "Loss at step 6300: 0.030006\n",
            "Loss at step 6310: 0.028142\n",
            "Loss at step 6320: 0.030856\n",
            "Loss at step 6330: 0.016908\n",
            "Loss at step 6340: 0.027274\n",
            "Loss at step 6350: 0.046017\n",
            "Loss at step 6360: 0.041648\n",
            "Loss at step 6370: 0.019396\n",
            "Loss at step 6380: 0.014619\n",
            "Loss at step 6390: 0.033706\n",
            "Loss at step 6400: 0.035459\n",
            "Loss at step 6410: 0.082987\n",
            "Loss at step 6420: 0.057286\n",
            "Loss at step 6430: 0.030057\n",
            "Loss at step 6440: 0.025141\n",
            "Loss at step 6450: 0.032471\n",
            "Loss at step 6460: 0.067848\n",
            "Loss at step 6470: 0.068622\n",
            "Loss at step 6480: 0.064731\n",
            "Loss at step 6490: 0.047692\n",
            "Loss at step 6500: 0.021259\n",
            "Loss at step 6510: 0.010457\n",
            "Loss at step 6520: 0.046211\n",
            "Loss at step 6530: 0.053900\n",
            "Loss at step 6540: 0.042101\n",
            "Loss at step 6550: 0.010853\n",
            "Loss at step 6560: 0.064305\n",
            "Loss at step 6570: 0.020840\n",
            "Loss at step 6580: 0.042708\n",
            "Loss at step 6590: 0.017299\n",
            "Loss at step 6600: 0.021196\n",
            "Loss at step 6610: 0.005934\n",
            "Loss at step 6620: 0.063249\n",
            "Loss at step 6630: 0.005903\n",
            "Loss at step 6640: 0.033513\n",
            "Loss at step 6650: 0.023086\n",
            "Loss at step 6660: 0.009323\n",
            "Loss at step 6670: 0.038316\n",
            "Loss at step 6680: 0.013143\n",
            "Loss at step 6690: 0.031057\n",
            "Loss at step 6700: 0.012293\n",
            "Loss at step 6710: 0.030064\n",
            "Loss at step 6720: 0.041768\n",
            "Loss at step 6730: 0.040684\n",
            "Loss at step 6740: 0.032548\n",
            "Loss at step 6750: 0.029579\n",
            "Loss at step 6760: 0.021858\n",
            "Loss at step 6770: 0.043458\n",
            "Loss at step 6780: 0.009103\n",
            "Loss at step 6790: 0.029213\n",
            "Loss at step 6800: 0.027227\n",
            "Loss at step 6810: 0.023417\n",
            "Loss at step 6820: 0.043196\n",
            "Loss at step 6830: 0.039310\n",
            "Loss at step 6840: 0.018161\n",
            "Loss at step 6850: 0.025556\n",
            "Loss at step 6860: 0.018040\n",
            "Loss at step 6870: 0.031855\n",
            "Loss at step 6880: 0.031293\n",
            "Loss at step 6890: 0.038627\n",
            "Loss at step 6900: 0.035018\n",
            "Loss at step 6910: 0.041369\n",
            "Loss at step 6920: 0.040289\n",
            "Loss at step 6930: 0.018306\n",
            "Loss at step 6940: 0.043708\n",
            "Loss at step 6950: 0.051991\n",
            "Loss at step 6960: 0.041529\n",
            "Loss at step 6970: 0.026714\n",
            "Loss at step 6980: 0.021968\n",
            "Loss at step 6990: 0.035467\n",
            "Loss at step 7000: 0.052316\n",
            "Loss at step 7010: 0.026052\n",
            "Loss at step 7020: 0.082769\n",
            "Loss at step 7030: 0.052304\n",
            "Loss at step 7040: 0.027893\n",
            "Loss at step 7050: 0.032621\n",
            "Loss at step 7060: 0.049640\n",
            "Loss at step 7070: 0.031931\n",
            "Loss at step 7080: 0.030480\n",
            "Loss at step 7090: 0.022344\n",
            "Loss at step 7100: 0.048513\n",
            "Loss at step 7110: 0.051044\n",
            "Loss at step 7120: 0.017781\n",
            "Loss at step 7130: 0.012717\n",
            "Loss at step 7140: 0.075715\n",
            "Loss at step 7150: 0.062467\n",
            "Loss at step 7160: 0.028414\n",
            "Loss at step 7170: 0.030511\n",
            "Loss at step 7180: 0.065716\n",
            "Loss at step 7190: 0.088843\n",
            "Loss at step 7200: 0.043282\n",
            "Loss at step 7210: 0.031400\n",
            "Loss at step 7220: 0.020214\n",
            "Loss at step 7230: 0.019058\n",
            "Loss at step 7240: 0.014161\n",
            "Loss at step 7250: 0.056143\n",
            "Loss at step 7260: 0.030248\n",
            "Loss at step 7270: 0.148788\n",
            "Loss at step 7280: 0.036401\n",
            "Loss at step 7290: 0.010830\n",
            "Loss at step 7300: 0.026378\n",
            "Loss at step 7310: 0.030967\n",
            "Loss at step 7320: 0.040341\n",
            "Loss at step 7330: 0.008066\n",
            "Loss at step 7340: 0.013077\n",
            "Loss at step 7350: 0.031020\n",
            "Loss at step 7360: 0.032452\n",
            "Loss at step 7370: 0.032901\n",
            "Loss at step 7380: 0.014006\n",
            "Loss at step 7390: 0.055958\n",
            "Loss at step 7400: 0.011160\n",
            "Loss at step 7410: 0.016563\n",
            "Loss at step 7420: 0.026045\n",
            "Loss at step 7430: 0.027472\n",
            "Loss at step 7440: 0.071931\n",
            "Loss at step 7450: 0.028476\n",
            "Loss at step 7460: 0.024069\n",
            "Loss at step 7470: 0.053190\n",
            "Loss at step 7480: 0.021547\n",
            "Loss at step 7490: 0.041207\n",
            "Loss at step 7500: 0.030430\n",
            "Loss at step 7510: 0.057279\n",
            "Loss at step 7520: 0.046481\n",
            "Loss at step 7530: 0.035545\n",
            "Loss at step 7540: 0.029434\n",
            "Loss at step 7550: 0.016096\n",
            "Loss at step 7560: 0.057482\n",
            "Loss at step 7570: 0.015243\n",
            "Loss at step 7580: 0.038925\n",
            "Loss at step 7590: 0.017934\n",
            "Loss at step 7600: 0.044508\n",
            "Loss at step 7610: 0.083405\n",
            "Loss at step 7620: 0.024653\n",
            "Loss at step 7630: 0.027198\n",
            "Loss at step 7640: 0.056284\n",
            "Loss at step 7650: 0.037765\n",
            "Loss at step 7660: 0.014341\n",
            "Loss at step 7670: 0.025228\n",
            "Loss at step 7680: 0.042539\n",
            "Loss at step 7690: 0.011854\n",
            "Loss at step 7700: 0.033733\n",
            "Loss at step 7710: 0.023206\n",
            "Loss at step 7720: 0.013872\n",
            "Loss at step 7730: 0.008129\n",
            "Loss at step 7740: 0.026441\n",
            "Loss at step 7750: 0.043857\n",
            "Loss at step 7760: 0.068219\n",
            "Loss at step 7770: 0.025198\n",
            "Loss at step 7780: 0.085752\n",
            "Loss at step 7790: 0.032618\n",
            "Loss at step 7800: 0.006702\n",
            "Loss at step 7810: 0.031392\n",
            "Loss at step 7820: 0.010490\n",
            "Loss at step 7830: 0.011646\n",
            "Loss at step 7840: 0.027896\n",
            "Loss at step 7850: 0.012631\n",
            "Loss at step 7860: 0.055088\n",
            "Loss at step 7870: 0.018807\n",
            "Loss at step 7880: 0.064172\n",
            "Loss at step 7890: 0.014926\n",
            "Loss at step 7900: 0.049356\n",
            "Loss at step 7910: 0.043234\n",
            "Loss at step 7920: 0.033081\n",
            "Loss at step 7930: 0.034388\n",
            "Loss at step 7940: 0.026653\n",
            "Loss at step 7950: 0.031640\n",
            "Loss at step 7960: 0.056348\n",
            "Loss at step 7970: 0.015359\n",
            "Loss at step 7980: 0.103525\n",
            "Loss at step 7990: 0.011562\n",
            "Loss at step 8000: 0.042716\n",
            "Loss at step 8010: 0.035660\n",
            "Loss at step 8020: 0.025624\n",
            "Loss at step 8030: 0.024427\n",
            "Loss at step 8040: 0.013351\n",
            "Loss at step 8050: 0.018743\n",
            "Loss at step 8060: 0.049127\n",
            "Loss at step 8070: 0.049644\n",
            "Loss at step 8080: 0.029000\n",
            "Loss at step 8090: 0.037834\n",
            "Loss at step 8100: 0.037304\n",
            "Loss at step 8110: 0.049632\n",
            "Loss at step 8120: 0.026431\n",
            "Loss at step 8130: 0.007759\n",
            "Loss at step 8140: 0.006845\n",
            "Loss at step 8150: 0.039690\n",
            "Loss at step 8160: 0.020679\n",
            "Loss at step 8170: 0.041900\n",
            "Loss at step 8180: 0.027738\n",
            "Loss at step 8190: 0.015328\n",
            "Loss at step 8200: 0.019699\n",
            "Loss at step 8210: 0.010570\n",
            "Loss at step 8220: 0.047332\n",
            "Loss at step 8230: 0.017662\n",
            "Loss at step 8240: 0.032386\n",
            "Loss at step 8250: 0.015934\n",
            "Loss at step 8260: 0.047717\n",
            "Loss at step 8270: 0.015599\n",
            "Loss at step 8280: 0.051361\n",
            "Loss at step 8290: 0.007741\n",
            "Loss at step 8300: 0.063910\n",
            "Loss at step 8310: 0.047199\n",
            "Loss at step 8320: 0.051163\n",
            "Loss at step 8330: 0.060640\n",
            "Loss at step 8340: 0.010788\n",
            "Loss at step 8350: 0.024819\n",
            "Loss at step 8360: 0.040424\n",
            "Loss at step 8370: 0.028175\n",
            "Loss at step 8380: 0.017000\n",
            "Loss at step 8390: 0.034702\n",
            "Loss at step 8400: 0.056644\n",
            "Loss at step 8410: 0.034014\n",
            "Loss at step 8420: 0.049753\n",
            "Loss at step 8430: 0.014670\n",
            "Loss at step 8440: 0.019301\n",
            "Loss at step 8450: 0.035191\n",
            "Loss at step 8460: 0.049912\n",
            "Loss at step 8470: 0.021424\n",
            "Loss at step 8480: 0.026815\n",
            "Loss at step 8490: 0.034409\n",
            "Loss at step 8500: 0.052712\n",
            "Loss at step 8510: 0.025599\n",
            "Loss at step 8520: 0.046721\n",
            "Loss at step 8530: 0.020330\n",
            "Loss at step 8540: 0.016608\n",
            "Loss at step 8550: 0.025063\n",
            "Loss at step 8560: 0.018979\n",
            "Loss at step 8570: 0.026233\n",
            "Loss at step 8580: 0.028869\n",
            "Loss at step 8590: 0.028258\n",
            "Loss at step 8600: 0.095009\n",
            "Loss at step 8610: 0.023824\n",
            "Loss at step 8620: 0.023610\n",
            "Loss at step 8630: 0.011748\n",
            "Loss at step 8640: 0.030488\n",
            "Loss at step 8650: 0.029798\n",
            "Loss at step 8660: 0.041649\n",
            "Loss at step 8670: 0.009811\n",
            "Loss at step 8680: 0.026040\n",
            "Loss at step 8690: 0.037277\n",
            "Loss at step 8700: 0.168593\n",
            "Loss at step 8710: 0.020562\n",
            "Loss at step 8720: 0.036310\n",
            "Loss at step 8730: 0.023072\n",
            "Loss at step 8740: 0.032153\n",
            "Loss at step 8750: 0.010599\n",
            "Loss at step 8760: 0.013210\n",
            "Loss at step 8770: 0.043891\n",
            "Loss at step 8780: 0.021511\n",
            "Loss at step 8790: 0.020095\n",
            "Loss at step 8800: 0.036294\n",
            "Loss at step 8810: 0.014574\n",
            "Loss at step 8820: 0.060788\n",
            "Loss at step 8830: 0.038645\n",
            "Loss at step 8840: 0.020276\n",
            "Loss at step 8850: 0.012053\n",
            "Loss at step 8860: 0.046179\n",
            "Loss at step 8870: 0.044089\n",
            "Loss at step 8880: 0.041963\n",
            "Loss at step 8890: 0.047630\n",
            "Loss at step 8900: 0.086890\n",
            "Loss at step 8910: 0.029775\n",
            "Loss at step 8920: 0.009481\n",
            "Loss at step 8930: 0.015652\n",
            "Loss at step 8940: 0.016743\n",
            "Loss at step 8950: 0.038715\n",
            "Loss at step 8960: 0.027704\n",
            "Loss at step 8970: 0.016952\n",
            "Loss at step 8980: 0.018078\n",
            "Loss at step 8990: 0.038961\n",
            "Loss at step 9000: 0.051089\n",
            "Loss at step 9010: 0.059640\n",
            "Loss at step 9020: 0.007617\n",
            "Loss at step 9030: 0.097008\n",
            "Loss at step 9040: 0.033212\n",
            "Loss at step 9050: 0.020979\n",
            "Loss at step 9060: 0.014655\n",
            "Loss at step 9070: 0.032492\n",
            "Loss at step 9080: 0.037042\n",
            "Loss at step 9090: 0.022219\n",
            "Loss at step 9100: 0.037902\n",
            "Loss at step 9110: 0.070362\n",
            "Loss at step 9120: 0.033515\n",
            "Loss at step 9130: 0.025269\n",
            "Loss at step 9140: 0.035187\n",
            "Loss at step 9150: 0.013913\n",
            "Loss at step 9160: 0.025038\n",
            "Loss at step 9170: 0.018836\n",
            "Loss at step 9180: 0.032690\n",
            "Loss at step 9190: 0.009780\n",
            "Loss at step 9200: 0.037685\n",
            "Loss at step 9210: 0.056849\n",
            "Loss at step 9220: 0.044523\n",
            "Loss at step 9230: 0.031633\n",
            "Loss at step 9240: 0.024197\n",
            "Loss at step 9250: 0.016337\n",
            "Loss at step 9260: 0.023734\n",
            "Loss at step 9270: 0.016730\n",
            "Loss at step 9280: 0.047167\n",
            "Loss at step 9290: 0.042372\n",
            "Loss at step 9300: 0.026289\n",
            "Loss at step 9310: 0.016535\n",
            "Loss at step 9320: 0.113888\n",
            "Loss at step 9330: 0.066638\n",
            "Loss at step 9340: 0.080556\n",
            "Loss at step 9350: 0.023070\n",
            "Loss at step 9360: 0.008482\n",
            "Loss at step 9370: 0.025418\n",
            "Loss at step 9380: 0.031699\n",
            "Loss at step 9390: 0.040057\n",
            "Loss at step 9400: 0.024525\n",
            "Loss at step 9410: 0.054755\n",
            "Loss at step 9420: 0.034038\n",
            "Loss at step 9430: 0.042977\n",
            "Loss at step 9440: 0.011016\n",
            "Loss at step 9450: 0.044513\n",
            "Loss at step 9460: 0.019608\n",
            "Loss at step 9470: 0.027458\n",
            "Loss at step 9480: 0.022897\n",
            "Loss at step 9490: 0.046759\n",
            "Loss at step 9500: 0.089988\n",
            "Loss at step 9510: 0.133063\n",
            "Loss at step 9520: 0.028720\n",
            "Loss at step 9530: 0.048276\n",
            "Loss at step 9540: 0.043915\n",
            "Loss at step 9550: 0.031978\n",
            "Loss at step 9560: 0.015880\n",
            "Loss at step 9570: 0.031979\n",
            "Loss at step 9580: 0.114898\n",
            "Loss at step 9590: 0.008087\n",
            "Loss at step 9600: 0.035638\n",
            "Loss at step 9610: 0.027250\n",
            "Loss at step 9620: 0.054313\n",
            "Loss at step 9630: 0.058459\n",
            "Loss at step 9640: 0.019215\n",
            "Loss at step 9650: 0.020225\n",
            "Loss at step 9660: 0.012394\n",
            "Loss at step 9670: 0.016189\n",
            "Loss at step 9680: 0.033152\n",
            "Loss at step 9690: 0.036920\n",
            "Loss at step 9700: 0.099808\n",
            "Loss at step 9710: 0.026884\n",
            "Loss at step 9720: 0.034577\n",
            "Loss at step 9730: 0.030957\n",
            "Loss at step 9740: 0.014579\n",
            "Loss at step 9750: 0.022139\n",
            "Loss at step 9760: 0.044383\n",
            "Loss at step 9770: 0.032151\n",
            "Loss at step 9780: 0.043155\n",
            "Loss at step 9790: 0.038176\n",
            "Loss at step 9800: 0.043971\n",
            "Loss at step 9810: 0.030662\n",
            "Loss at step 9820: 0.039184\n",
            "Loss at step 9830: 0.027880\n",
            "Loss at step 9840: 0.013666\n",
            "Loss at step 9850: 0.060712\n",
            "Loss at step 9860: 0.025457\n",
            "Loss at step 9870: 0.022266\n",
            "Loss at step 9880: 0.039330\n",
            "Loss at step 9890: 0.021675\n",
            "Loss at step 9900: 0.029154\n",
            "Loss at step 9910: 0.092345\n",
            "Loss at step 9920: 0.017517\n",
            "Loss at step 9930: 0.026876\n",
            "Loss at step 9940: 0.047929\n",
            "Loss at step 9950: 0.094158\n",
            "Loss at step 9960: 0.046095\n",
            "Loss at step 9970: 0.010660\n",
            "Loss at step 9980: 0.021604\n",
            "Loss at step 9990: 0.015229\n",
            "Loss at step 10000: 0.046026\n",
            "Loss at step 10010: 0.008181\n",
            "Loss at step 10020: 0.029995\n",
            "Loss at step 10030: 0.015882\n",
            "Loss at step 10040: 0.017629\n",
            "Loss at step 10050: 0.019944\n",
            "Loss at step 10060: 0.089411\n",
            "Loss at step 10070: 0.028585\n",
            "Loss at step 10080: 0.048313\n",
            "Loss at step 10090: 0.017843\n",
            "Loss at step 10100: 0.011567\n",
            "Loss at step 10110: 0.031372\n",
            "Loss at step 10120: 0.044149\n",
            "Loss at step 10130: 0.030156\n",
            "Loss at step 10140: 0.040030\n",
            "Loss at step 10150: 0.019352\n",
            "Loss at step 10160: 0.048534\n",
            "Loss at step 10170: 0.017196\n",
            "Loss at step 10180: 0.052958\n",
            "Loss at step 10190: 0.048356\n",
            "Loss at step 10200: 0.022397\n",
            "Loss at step 10210: 0.037211\n",
            "Loss at step 10220: 0.047926\n",
            "Loss at step 10230: 0.028511\n",
            "Loss at step 10240: 0.044257\n",
            "Loss at step 10250: 0.013449\n",
            "Loss at step 10260: 0.030766\n",
            "Loss at step 10270: 0.048334\n",
            "Loss at step 10280: 0.030299\n",
            "Loss at step 10290: 0.009085\n",
            "Loss at step 10300: 0.022070\n",
            "Loss at step 10310: 0.026266\n",
            "Loss at step 10320: 0.039354\n",
            "Loss at step 10330: 0.058017\n",
            "Loss at step 10340: 0.073715\n",
            "Loss at step 10350: 0.100126\n",
            "Loss at step 10360: 0.010785\n",
            "Loss at step 10370: 0.016208\n",
            "Loss at step 10380: 0.025508\n",
            "Loss at step 10390: 0.011010\n",
            "Loss at step 10400: 0.045865\n",
            "Loss at step 10410: 0.033562\n",
            "Loss at step 10420: 0.018376\n",
            "Loss at step 10430: 0.049166\n",
            "Loss at step 10440: 0.043633\n",
            "Loss at step 10450: 0.018596\n",
            "Loss at step 10460: 0.007567\n",
            "Loss at step 10470: 0.024828\n",
            "Loss at step 10480: 0.019550\n",
            "Loss at step 10490: 0.050433\n",
            "Loss at step 10500: 0.014999\n",
            "Loss at step 10510: 0.017036\n",
            "Loss at step 10520: 0.066391\n",
            "Loss at step 10530: 0.012731\n",
            "Loss at step 10540: 0.013426\n",
            "Loss at step 10550: 0.044249\n",
            "Loss at step 10560: 0.025402\n",
            "Loss at step 10570: 0.040536\n",
            "Loss at step 10580: 0.027857\n",
            "Loss at step 10590: 0.017352\n",
            "Loss at step 10600: 0.019852\n",
            "Loss at step 10610: 0.062671\n",
            "Loss at step 10620: 0.022761\n",
            "Loss at step 10630: 0.032718\n",
            "Loss at step 10640: 0.038950\n",
            "Loss at step 10650: 0.037296\n",
            "Loss at step 10660: 0.021882\n",
            "Loss at step 10670: 0.022084\n",
            "Loss at step 10680: 0.071646\n",
            "Loss at step 10690: 0.015199\n",
            "Loss at step 10700: 0.014311\n",
            "Loss at step 10710: 0.023790\n",
            "Loss at step 10720: 0.045921\n",
            "Loss at step 10730: 0.011558\n",
            "Loss at step 10740: 0.027120\n",
            "Loss at step 10750: 0.070284\n",
            "Loss at step 10760: 0.040356\n",
            "Loss at step 10770: 0.053739\n",
            "Loss at step 10780: 0.021820\n",
            "Loss at step 10790: 0.012748\n",
            "Loss at step 10800: 0.030421\n",
            "Loss at step 10810: 0.055590\n",
            "Loss at step 10820: 0.013504\n",
            "Loss at step 10830: 0.058048\n",
            "Loss at step 10840: 0.019101\n",
            "Loss at step 10850: 0.031597\n",
            "Loss at step 10860: 0.026192\n",
            "Loss at step 10870: 0.008095\n",
            "Loss at step 10880: 0.008537\n",
            "Loss at step 10890: 0.072301\n",
            "Loss at step 10900: 0.025643\n",
            "Loss at step 10910: 0.056123\n",
            "Loss at step 10920: 0.030595\n",
            "Loss at step 10930: 0.007234\n",
            "Loss at step 10940: 0.024833\n",
            "Loss at step 10950: 0.025071\n",
            "Loss at step 10960: 0.071164\n",
            "Loss at step 10970: 0.062891\n",
            "Loss at step 10980: 0.032722\n",
            "Loss at step 10990: 0.007756\n",
            "Loss at step 11000: 0.029498\n",
            "Loss at step 11010: 0.047473\n",
            "Loss at step 11020: 0.012673\n",
            "Loss at step 11030: 0.014001\n",
            "Loss at step 11040: 0.028398\n",
            "Loss at step 11050: 0.056252\n",
            "Loss at step 11060: 0.027021\n",
            "Loss at step 11070: 0.029492\n",
            "Loss at step 11080: 0.009584\n",
            "Loss at step 11090: 0.017611\n",
            "Loss at step 11100: 0.030216\n",
            "Loss at step 11110: 0.018848\n",
            "Loss at step 11120: 0.028250\n",
            "Loss at step 11130: 0.023302\n",
            "Loss at step 11140: 0.073740\n",
            "Loss at step 11150: 0.033381\n",
            "Loss at step 11160: 0.036531\n",
            "Loss at step 11170: 0.050810\n",
            "Loss at step 11180: 0.054071\n",
            "Loss at step 11190: 0.021869\n",
            "Loss at step 11200: 0.019839\n",
            "Loss at step 11210: 0.027478\n",
            "Loss at step 11220: 0.015457\n",
            "Loss at step 11230: 0.018047\n",
            "Loss at step 11240: 0.034477\n",
            "Loss at step 11250: 0.032762\n",
            "Loss at step 11260: 0.038036\n",
            "Loss at step 11270: 0.062453\n",
            "Loss at step 11280: 0.011285\n",
            "Loss at step 11290: 0.053484\n",
            "Loss at step 11300: 0.024871\n",
            "Loss at step 11310: 0.070105\n",
            "Loss at step 11320: 0.013206\n",
            "Loss at step 11330: 0.032392\n",
            "Loss at step 11340: 0.011177\n",
            "Loss at step 11350: 0.009122\n",
            "Loss at step 11360: 0.011641\n",
            "Loss at step 11370: 0.052343\n",
            "Loss at step 11380: 0.076937\n",
            "Loss at step 11390: 0.073687\n",
            "Loss at step 11400: 0.025784\n",
            "Loss at step 11410: 0.031717\n",
            "Loss at step 11420: 0.037547\n",
            "Loss at step 11430: 0.029534\n",
            "Loss at step 11440: 0.022604\n",
            "Loss at step 11450: 0.032366\n",
            "Loss at step 11460: 0.025566\n",
            "Loss at step 11470: 0.027516\n",
            "Loss at step 11480: 0.016329\n",
            "Loss at step 11490: 0.015395\n",
            "Loss at step 11500: 0.025124\n",
            "Loss at step 11510: 0.010784\n",
            "Loss at step 11520: 0.044671\n",
            "Loss at step 11530: 0.049063\n",
            "Loss at step 11540: 0.006957\n",
            "Loss at step 11550: 0.025768\n",
            "Loss at step 11560: 0.009808\n",
            "Loss at step 11570: 0.054771\n",
            "Loss at step 11580: 0.018093\n",
            "Loss at step 11590: 0.014458\n",
            "Loss at step 11600: 0.022353\n",
            "Loss at step 11610: 0.043146\n",
            "Loss at step 11620: 0.016996\n",
            "Loss at step 11630: 0.033670\n",
            "Loss at step 11640: 0.006238\n",
            "Loss at step 11650: 0.017043\n",
            "Loss at step 11660: 0.012730\n",
            "Loss at step 11670: 0.052399\n",
            "Loss at step 11680: 0.017286\n",
            "Loss at step 11690: 0.030009\n",
            "Loss at step 11700: 0.054441\n",
            "Loss at step 11710: 0.031892\n",
            "Loss at step 11720: 0.017367\n",
            "Loss at step 11730: 0.022812\n",
            "Loss at step 11740: 0.018406\n",
            "Loss at step 11750: 0.002887\n",
            "Loss at step 11760: 0.037476\n",
            "Loss at step 11770: 0.045724\n",
            "Loss at step 11780: 0.088513\n",
            "Loss at step 11790: 0.016102\n",
            "Loss at step 11800: 0.009043\n",
            "Loss at step 11810: 0.024465\n",
            "Loss at step 11820: 0.035892\n",
            "Loss at step 11830: 0.014629\n",
            "Loss at step 11840: 0.010595\n",
            "Loss at step 11850: 0.010600\n",
            "Loss at step 11860: 0.020891\n",
            "Loss at step 11870: 0.042397\n",
            "Loss at step 11880: 0.041543\n",
            "Loss at step 11890: 0.025448\n",
            "Loss at step 11900: 0.013158\n",
            "Loss at step 11910: 0.033664\n",
            "Loss at step 11920: 0.061264\n",
            "Loss at step 11930: 0.028049\n",
            "Loss at step 11940: 0.014106\n",
            "Loss at step 11950: 0.025150\n",
            "Loss at step 11960: 0.036691\n",
            "Loss at step 11970: 0.068271\n",
            "Loss at step 11980: 0.023326\n",
            "Loss at step 11990: 0.048875\n",
            "Loss at step 12000: 0.065894\n",
            "Loss at step 12010: 0.037011\n",
            "Loss at step 12020: 0.061936\n",
            "Loss at step 12030: 0.045237\n",
            "Loss at step 12040: 0.012443\n",
            "Loss at step 12050: 0.023393\n",
            "Loss at step 12060: 0.047959\n",
            "Loss at step 12070: 0.034157\n",
            "Loss at step 12080: 0.037174\n",
            "Loss at step 12090: 0.041176\n",
            "Loss at step 12100: 0.013909\n",
            "Loss at step 12110: 0.018923\n",
            "Loss at step 12120: 0.040425\n",
            "Loss at step 12130: 0.029089\n",
            "Loss at step 12140: 0.041499\n",
            "Loss at step 12150: 0.021943\n",
            "Loss at step 12160: 0.014561\n",
            "Loss at step 12170: 0.038427\n",
            "Loss at step 12180: 0.019486\n",
            "Loss at step 12190: 0.036564\n",
            "Loss at step 12200: 0.024337\n",
            "Loss at step 12210: 0.039873\n",
            "Loss at step 12220: 0.020252\n",
            "Loss at step 12230: 0.077936\n",
            "Loss at step 12240: 0.038117\n",
            "Loss at step 12250: 0.020162\n",
            "Loss at step 12260: 0.025894\n",
            "Loss at step 12270: 0.024667\n",
            "Loss at step 12280: 0.036356\n",
            "Loss at step 12290: 0.025330\n",
            "Loss at step 12300: 0.051267\n",
            "Loss at step 12310: 0.014414\n",
            "Loss at step 12320: 0.034794\n",
            "Loss at step 12330: 0.033522\n",
            "Loss at step 12340: 0.023546\n",
            "Loss at step 12350: 0.011472\n",
            "Loss at step 12360: 0.044697\n",
            "Loss at step 12370: 0.033080\n",
            "Loss at step 12380: 0.021564\n",
            "Loss at step 12390: 0.027092\n",
            "Loss at step 12400: 0.016974\n",
            "Loss at step 12410: 0.029400\n",
            "Loss at step 12420: 0.040409\n",
            "Loss at step 12430: 0.013453\n",
            "Loss at step 12440: 0.034895\n",
            "Loss at step 12450: 0.008410\n",
            "Loss at step 12460: 0.101961\n",
            "Loss at step 12470: 0.024853\n",
            "Loss at step 12480: 0.012311\n",
            "Loss at step 12490: 0.023295\n",
            "Loss at step 12500: 0.009418\n",
            "Loss at step 12510: 0.019409\n",
            "Loss at step 12520: 0.038616\n",
            "Loss at step 12530: 0.064342\n",
            "Loss at step 12540: 0.029798\n",
            "Loss at step 12550: 0.048575\n",
            "Loss at step 12560: 0.041720\n",
            "Loss at step 12570: 0.013773\n",
            "Loss at step 12580: 0.068324\n",
            "Loss at step 12590: 0.031701\n",
            "Loss at step 12600: 0.018402\n",
            "Loss at step 12610: 0.093046\n",
            "Loss at step 12620: 0.028666\n",
            "Loss at step 12630: 0.023861\n",
            "Loss at step 12640: 0.025174\n",
            "Loss at step 12650: 0.047284\n",
            "Loss at step 12660: 0.042151\n",
            "Loss at step 12670: 0.011817\n",
            "Loss at step 12680: 0.008937\n",
            "Loss at step 12690: 0.032563\n",
            "Loss at step 12700: 0.020860\n",
            "Loss at step 12710: 0.050973\n",
            "Loss at step 12720: 0.024933\n",
            "Loss at step 12730: 0.057060\n",
            "Loss at step 12740: 0.095087\n",
            "Loss at step 12750: 0.018186\n",
            "Loss at step 12760: 0.037336\n",
            "Loss at step 12770: 0.068285\n",
            "Loss at step 12780: 0.039320\n",
            "Loss at step 12790: 0.017275\n",
            "Loss at step 12800: 0.021773\n",
            "Loss at step 12810: 0.066480\n",
            "Loss at step 12820: 0.015228\n",
            "Loss at step 12830: 0.020759\n",
            "Loss at step 12840: 0.037017\n",
            "Loss at step 12850: 0.070915\n",
            "Loss at step 12860: 0.018901\n",
            "Loss at step 12870: 0.061946\n",
            "Loss at step 12880: 0.022355\n",
            "Loss at step 12890: 0.021807\n",
            "Loss at step 12900: 0.033489\n",
            "Loss at step 12910: 0.006429\n",
            "Loss at step 12920: 0.063085\n",
            "Loss at step 12930: 0.042532\n",
            "Loss at step 12940: 0.025739\n",
            "Loss at step 12950: 0.034948\n",
            "Loss at step 12960: 0.015608\n",
            "Loss at step 12970: 0.021828\n",
            "Loss at step 12980: 0.048704\n",
            "Loss at step 12990: 0.025987\n",
            "Loss at step 13000: 0.020812\n",
            "Loss at step 13010: 0.015137\n",
            "Loss at step 13020: 0.026073\n",
            "Loss at step 13030: 0.020657\n",
            "Loss at step 13040: 0.021326\n",
            "Loss at step 13050: 0.056488\n",
            "Loss at step 13060: 0.019581\n",
            "Loss at step 13070: 0.031953\n",
            "Loss at step 13080: 0.023707\n",
            "Loss at step 13090: 0.015209\n",
            "Loss at step 13100: 0.026970\n",
            "Loss at step 13110: 0.172886\n",
            "Loss at step 13120: 0.008070\n",
            "Loss at step 13130: 0.029903\n",
            "Loss at step 13140: 0.010417\n",
            "Loss at step 13150: 0.012584\n",
            "Loss at step 13160: 0.034226\n",
            "Loss at step 13170: 0.008711\n",
            "Loss at step 13180: 0.046515\n",
            "Loss at step 13190: 0.027976\n",
            "Loss at step 13200: 0.063418\n",
            "Loss at step 13210: 0.038586\n",
            "Loss at step 13220: 0.045513\n",
            "Loss at step 13230: 0.014115\n",
            "Loss at step 13240: 0.049599\n",
            "Loss at step 13250: 0.044347\n",
            "Loss at step 13260: 0.015832\n",
            "Loss at step 13270: 0.029188\n",
            "Loss at step 13280: 0.036226\n",
            "Loss at step 13290: 0.068208\n",
            "Loss at step 13300: 0.045574\n",
            "Loss at step 13310: 0.017435\n",
            "Loss at step 13320: 0.104994\n",
            "Loss at step 13330: 0.028991\n",
            "Loss at step 13340: 0.007521\n",
            "Loss at step 13350: 0.040286\n",
            "Loss at step 13360: 0.040620\n",
            "Loss at step 13370: 0.037906\n",
            "Loss at step 13380: 0.013764\n",
            "Loss at step 13390: 0.037555\n",
            "Loss at step 13400: 0.008931\n",
            "Loss at step 13410: 0.020889\n",
            "Loss at step 13420: 0.041015\n",
            "Loss at step 13430: 0.024107\n",
            "Loss at step 13440: 0.044710\n",
            "Loss at step 13450: 0.032228\n",
            "Loss at step 13460: 0.052563\n",
            "Loss at step 13470: 0.026615\n",
            "Loss at step 13480: 0.027651\n",
            "Loss at step 13490: 0.011561\n",
            "Loss at step 13500: 0.030433\n",
            "Loss at step 13510: 0.032265\n",
            "Loss at step 13520: 0.026147\n",
            "Loss at step 13530: 0.030695\n",
            "Loss at step 13540: 0.009873\n",
            "Loss at step 13550: 0.019157\n",
            "Loss at step 13560: 0.058220\n",
            "Loss at step 13570: 0.016682\n",
            "Loss at step 13580: 0.064191\n",
            "Loss at step 13590: 0.032642\n",
            "Loss at step 13600: 0.039041\n",
            "Loss at step 13610: 0.053540\n",
            "Loss at step 13620: 0.039765\n",
            "Loss at step 13630: 0.033863\n",
            "Loss at step 13640: 0.053207\n",
            "Loss at step 13650: 0.037257\n",
            "Loss at step 13660: 0.062185\n",
            "Loss at step 13670: 0.072509\n",
            "Loss at step 13680: 0.021921\n",
            "Loss at step 13690: 0.014632\n",
            "Loss at step 13700: 0.016449\n",
            "Loss at step 13710: 0.045454\n",
            "Loss at step 13720: 0.026607\n",
            "Loss at step 13730: 0.035655\n",
            "Loss at step 13740: 0.043765\n",
            "Loss at step 13750: 0.014659\n",
            "Loss at step 13760: 0.034683\n",
            "Loss at step 13770: 0.049079\n",
            "Loss at step 13780: 0.047213\n",
            "Loss at step 13790: 0.012000\n",
            "Loss at step 13800: 0.017555\n",
            "Loss at step 13810: 0.053422\n",
            "Loss at step 13820: 0.036346\n",
            "Loss at step 13830: 0.022278\n",
            "Loss at step 13840: 0.059460\n",
            "Loss at step 13850: 0.023895\n",
            "Loss at step 13860: 0.016099\n",
            "Loss at step 13870: 0.021494\n",
            "Loss at step 13880: 0.021416\n",
            "Loss at step 13890: 0.036624\n",
            "Loss at step 13900: 0.009922\n",
            "Loss at step 13910: 0.029410\n",
            "Loss at step 13920: 0.032202\n",
            "Loss at step 13930: 0.031524\n",
            "Loss at step 13940: 0.020794\n",
            "Loss at step 13950: 0.017524\n",
            "Loss at step 13960: 0.025672\n",
            "Loss at step 13970: 0.024180\n",
            "Loss at step 13980: 0.024051\n",
            "Loss at step 13990: 0.040924\n",
            "Loss at step 14000: 0.084713\n",
            "Loss at step 14010: 0.013886\n",
            "Loss at step 14020: 0.026443\n",
            "Loss at step 14030: 0.045383\n",
            "Loss at step 14040: 0.025245\n",
            "Loss at step 14050: 0.026143\n",
            "Loss at step 14060: 0.023158\n",
            "Loss at step 14070: 0.034111\n",
            "Loss at step 14080: 0.017299\n",
            "Loss at step 14090: 0.025533\n",
            "Loss at step 14100: 0.007913\n",
            "Loss at step 14110: 0.012550\n",
            "Loss at step 14120: 0.022152\n",
            "Loss at step 14130: 0.021333\n",
            "Loss at step 14140: 0.048004\n",
            "Loss at step 14150: 0.041461\n",
            "Loss at step 14160: 0.030203\n",
            "Loss at step 14170: 0.060523\n",
            "Loss at step 14180: 0.029991\n",
            "Loss at step 14190: 0.023569\n",
            "Loss at step 14200: 0.010096\n",
            "Loss at step 14210: 0.026254\n",
            "Loss at step 14220: 0.008671\n",
            "Loss at step 14230: 0.034979\n",
            "Loss at step 14240: 0.016124\n",
            "Loss at step 14250: 0.028089\n",
            "Loss at step 14260: 0.035201\n",
            "Loss at step 14270: 0.055648\n",
            "Loss at step 14280: 0.015182\n",
            "Loss at step 14290: 0.027115\n",
            "Loss at step 14300: 0.019713\n",
            "Loss at step 14310: 0.042184\n",
            "Loss at step 14320: 0.042203\n",
            "Loss at step 14330: 0.015476\n",
            "Loss at step 14340: 0.041400\n",
            "Loss at step 14350: 0.023007\n",
            "Loss at step 14360: 0.008382\n",
            "Loss at step 14370: 0.029422\n",
            "Loss at step 14380: 0.047495\n",
            "Loss at step 14390: 0.019905\n",
            "Loss at step 14400: 0.038036\n",
            "Loss at step 14410: 0.012576\n",
            "Loss at step 14420: 0.025193\n",
            "Loss at step 14430: 0.062086\n",
            "Loss at step 14440: 0.050468\n",
            "Loss at step 14450: 0.022500\n",
            "Loss at step 14460: 0.034203\n",
            "Loss at step 14470: 0.030464\n",
            "Loss at step 14480: 0.020028\n",
            "Loss at step 14490: 0.042203\n",
            "Loss at step 14500: 0.048075\n",
            "Loss at step 14510: 0.026137\n",
            "Loss at step 14520: 0.064369\n",
            "Loss at step 14530: 0.060221\n",
            "Loss at step 14540: 0.023962\n",
            "Loss at step 14550: 0.016858\n",
            "Loss at step 14560: 0.046146\n",
            "Loss at step 14570: 0.022107\n",
            "Loss at step 14580: 0.062231\n",
            "Loss at step 14590: 0.012489\n",
            "Loss at step 14600: 0.052781\n",
            "Loss at step 14610: 0.034966\n",
            "Loss at step 14620: 0.019385\n",
            "Loss at step 14630: 0.083270\n",
            "Loss at step 14640: 0.014534\n",
            "Loss at step 14650: 0.025538\n",
            "Loss at step 14660: 0.022125\n",
            "Loss at step 14670: 0.018168\n",
            "Loss at step 14680: 0.094611\n",
            "Loss at step 14690: 0.044295\n",
            "Loss at step 14700: 0.059092\n",
            "Loss at step 14710: 0.019519\n",
            "Loss at step 14720: 0.042591\n",
            "Loss at step 14730: 0.025140\n",
            "Loss at step 14740: 0.036757\n",
            "Loss at step 14750: 0.016913\n",
            "Loss at step 14760: 0.025180\n",
            "Loss at step 14770: 0.022287\n",
            "Loss at step 14780: 0.009771\n",
            "Loss at step 14790: 0.017263\n",
            "Loss at step 14800: 0.053812\n",
            "Loss at step 14810: 0.039197\n",
            "Loss at step 14820: 0.019431\n",
            "Loss at step 14830: 0.028060\n",
            "Loss at step 14840: 0.019670\n",
            "Loss at step 14850: 0.058299\n",
            "Loss at step 14860: 0.015196\n",
            "Loss at step 14870: 0.020714\n",
            "Loss at step 14880: 0.042773\n",
            "Loss at step 14890: 0.024564\n",
            "Loss at step 14900: 0.020306\n",
            "Loss at step 14910: 0.084693\n",
            "Loss at step 14920: 0.023495\n",
            "Loss at step 14930: 0.026354\n",
            "Loss at step 14940: 0.046417\n",
            "Loss at step 14950: 0.009089\n",
            "Loss at step 14960: 0.014356\n",
            "Loss at step 14970: 0.031493\n",
            "Loss at step 14980: 0.059950\n",
            "Loss at step 14990: 0.064000\n",
            "Loss at step 15000: 0.025279\n",
            "Loss at step 15010: 0.028444\n",
            "Loss at step 15020: 0.028639\n",
            "Loss at step 15030: 0.046137\n",
            "Loss at step 15040: 0.051156\n",
            "Loss at step 15050: 0.038567\n",
            "Loss at step 15060: 0.008194\n",
            "Loss at step 15070: 0.039129\n",
            "Loss at step 15080: 0.018726\n",
            "Loss at step 15090: 0.059157\n",
            "Loss at step 15100: 0.031531\n",
            "Loss at step 15110: 0.047216\n",
            "Loss at step 15120: 0.026962\n",
            "Loss at step 15130: 0.009400\n",
            "Loss at step 15140: 0.026916\n",
            "Loss at step 15150: 0.042818\n",
            "Loss at step 15160: 0.022527\n",
            "Loss at step 15170: 0.013184\n",
            "Loss at step 15180: 0.015283\n",
            "Loss at step 15190: 0.041396\n",
            "Loss at step 15200: 0.076664\n",
            "Loss at step 15210: 0.039519\n",
            "Loss at step 15220: 0.022421\n",
            "Loss at step 15230: 0.015927\n",
            "Loss at step 15240: 0.018209\n",
            "Loss at step 15250: 0.022450\n",
            "Loss at step 15260: 0.025844\n",
            "Loss at step 15270: 0.031544\n",
            "Loss at step 15280: 0.025730\n",
            "Loss at step 15290: 0.057245\n",
            "Loss at step 15300: 0.100638\n",
            "Loss at step 15310: 0.033585\n",
            "Loss at step 15320: 0.017653\n",
            "Loss at step 15330: 0.053863\n",
            "Loss at step 15340: 0.046289\n",
            "Loss at step 15350: 0.045203\n",
            "Loss at step 15360: 0.019760\n",
            "Loss at step 15370: 0.029985\n",
            "Loss at step 15380: 0.052585\n",
            "Loss at step 15390: 0.019322\n",
            "Loss at step 15400: 0.030814\n",
            "Loss at step 15410: 0.058097\n",
            "Loss at step 15420: 0.097932\n",
            "Loss at step 15430: 0.039726\n",
            "Loss at step 15440: 0.041160\n",
            "Loss at step 15450: 0.051633\n",
            "Loss at step 15460: 0.014916\n",
            "Loss at step 15470: 0.045411\n",
            "Loss at step 15480: 0.052811\n",
            "Loss at step 15490: 0.018405\n",
            "Loss at step 15500: 0.019633\n",
            "Loss at step 15510: 0.071271\n",
            "Loss at step 15520: 0.022017\n",
            "Loss at step 15530: 0.079893\n",
            "Loss at step 15540: 0.045765\n",
            "Loss at step 15550: 0.026868\n",
            "Loss at step 15560: 0.032240\n",
            "Loss at step 15570: 0.032448\n",
            "Loss at step 15580: 0.058890\n",
            "Loss at step 15590: 0.026620\n",
            "Loss at step 15600: 0.088202\n",
            "Loss at step 15610: 0.032890\n",
            "Loss at step 15620: 0.015420\n",
            "Loss at step 15630: 0.032559\n",
            "Loss at step 15640: 0.023352\n",
            "Loss at step 15650: 0.016526\n",
            "Loss at step 15660: 0.019517\n",
            "Loss at step 15670: 0.068320\n",
            "Loss at step 15680: 0.023120\n",
            "Loss at step 15690: 0.032709\n",
            "Loss at step 15700: 0.022378\n",
            "Loss at step 15710: 0.018528\n",
            "Loss at step 15720: 0.028825\n",
            "Loss at step 15730: 0.024017\n",
            "Loss at step 15740: 0.068253\n",
            "Loss at step 15750: 0.022456\n",
            "Loss at step 15760: 0.088752\n",
            "Loss at step 15770: 0.051221\n",
            "Loss at step 15780: 0.018115\n",
            "Loss at step 15790: 0.039379\n",
            "Loss at step 15800: 0.047709\n",
            "Loss at step 15810: 0.085618\n",
            "Loss at step 15820: 0.014460\n",
            "Loss at step 15830: 0.032589\n",
            "Loss at step 15840: 0.023812\n",
            "Loss at step 15850: 0.020011\n",
            "Loss at step 15860: 0.040147\n",
            "Loss at step 15870: 0.006034\n",
            "Loss at step 15880: 0.021783\n",
            "Loss at step 15890: 0.021777\n",
            "Loss at step 15900: 0.009062\n",
            "Loss at step 15910: 0.019973\n",
            "Loss at step 15920: 0.026674\n",
            "Loss at step 15930: 0.038988\n",
            "Loss at step 15940: 0.014521\n",
            "Loss at step 15950: 0.020000\n",
            "Loss at step 15960: 0.051453\n",
            "Loss at step 15970: 0.018484\n",
            "Loss at step 15980: 0.022611\n",
            "Loss at step 15990: 0.027987\n",
            "Loss at step 16000: 0.008851\n",
            "Loss at step 16010: 0.029492\n",
            "Loss at step 16020: 0.017685\n",
            "Loss at step 16030: 0.022820\n",
            "Loss at step 16040: 0.028295\n",
            "Loss at step 16050: 0.026867\n",
            "Loss at step 16060: 0.010184\n",
            "Loss at step 16070: 0.076976\n",
            "Loss at step 16080: 0.022268\n",
            "Loss at step 16090: 0.024740\n",
            "Loss at step 16100: 0.008628\n",
            "Loss at step 16110: 0.048094\n",
            "Loss at step 16120: 0.017951\n",
            "Loss at step 16130: 0.033674\n",
            "Loss at step 16140: 0.038575\n",
            "Loss at step 16150: 0.043697\n",
            "Loss at step 16160: 0.057624\n",
            "Loss at step 16170: 0.015685\n",
            "Loss at step 16180: 0.021165\n",
            "Loss at step 16190: 0.037435\n",
            "Loss at step 16200: 0.048206\n",
            "Loss at step 16210: 0.032450\n",
            "Loss at step 16220: 0.045296\n",
            "Loss at step 16230: 0.026651\n",
            "Epoch Number: 1\n",
            "Loss at step 16240: 0.012728\n",
            "Loss at step 16250: 0.013613\n",
            "Loss at step 16260: 0.113409\n",
            "Loss at step 16270: 0.023288\n",
            "Loss at step 16280: 0.042611\n",
            "Loss at step 16290: 0.022514\n",
            "Loss at step 16300: 0.032512\n",
            "Loss at step 16310: 0.021891\n",
            "Loss at step 16320: 0.021425\n",
            "Loss at step 16330: 0.028673\n",
            "Loss at step 16340: 0.056765\n",
            "Loss at step 16350: 0.016862\n",
            "Loss at step 16360: 0.041278\n",
            "Loss at step 16370: 0.031482\n",
            "Loss at step 16380: 0.029772\n",
            "Loss at step 16390: 0.034299\n",
            "Loss at step 16400: 0.029079\n",
            "Loss at step 16410: 0.020395\n",
            "Loss at step 16420: 0.005377\n",
            "Loss at step 16430: 0.048839\n",
            "Loss at step 16440: 0.037842\n",
            "Loss at step 16450: 0.053854\n",
            "Loss at step 16460: 0.037922\n",
            "Loss at step 16470: 0.046837\n",
            "Loss at step 16480: 0.029026\n",
            "Loss at step 16490: 0.039319\n",
            "Loss at step 16500: 0.043660\n",
            "Loss at step 16510: 0.021027\n",
            "Loss at step 16520: 0.025455\n",
            "Loss at step 16530: 0.029446\n",
            "Loss at step 16540: 0.057433\n",
            "Loss at step 16550: 0.035557\n",
            "Loss at step 16560: 0.050784\n",
            "Loss at step 16570: 0.156122\n",
            "Loss at step 16580: 0.026071\n",
            "Loss at step 16590: 0.068547\n",
            "Loss at step 16600: 0.027421\n",
            "Loss at step 16610: 0.012965\n",
            "Loss at step 16620: 0.026653\n",
            "Loss at step 16630: 0.006182\n",
            "Loss at step 16640: 0.020308\n",
            "Loss at step 16650: 0.026736\n",
            "Loss at step 16660: 0.039380\n",
            "Loss at step 16670: 0.034999\n",
            "Loss at step 16680: 0.027826\n",
            "Loss at step 16690: 0.025142\n",
            "Loss at step 16700: 0.038898\n",
            "Loss at step 16710: 0.046477\n",
            "Loss at step 16720: 0.028385\n",
            "Loss at step 16730: 0.026141\n",
            "Loss at step 16740: 0.023774\n",
            "Loss at step 16750: 0.027985\n",
            "Loss at step 16760: 0.020365\n",
            "Loss at step 16770: 0.019388\n",
            "Loss at step 16780: 0.037693\n",
            "Loss at step 16790: 0.045268\n",
            "Loss at step 16800: 0.036423\n",
            "Loss at step 16810: 0.020185\n",
            "Loss at step 16820: 0.020134\n",
            "Loss at step 16830: 0.084480\n",
            "Loss at step 16840: 0.010455\n",
            "Loss at step 16850: 0.023200\n",
            "Loss at step 16860: 0.009793\n",
            "Loss at step 16870: 0.009550\n",
            "Loss at step 16880: 0.017360\n",
            "Loss at step 16890: 0.064452\n",
            "Loss at step 16900: 0.043134\n",
            "Loss at step 16910: 0.027653\n",
            "Loss at step 16920: 0.011691\n",
            "Loss at step 16930: 0.026260\n",
            "Loss at step 16940: 0.052013\n",
            "Loss at step 16950: 0.034045\n",
            "Loss at step 16960: 0.013254\n",
            "Loss at step 16970: 0.054374\n",
            "Loss at step 16980: 0.019159\n",
            "Loss at step 16990: 0.023275\n",
            "Loss at step 17000: 0.051708\n",
            "Loss at step 17010: 0.036493\n",
            "Loss at step 17020: 0.016297\n",
            "Loss at step 17030: 0.011536\n",
            "Loss at step 17040: 0.007528\n",
            "Loss at step 17050: 0.009220\n",
            "Loss at step 17060: 0.052816\n",
            "Loss at step 17070: 0.020544\n",
            "Loss at step 17080: 0.045482\n",
            "Loss at step 17090: 0.019613\n",
            "Loss at step 17100: 0.024447\n",
            "Loss at step 17110: 0.041827\n",
            "Loss at step 17120: 0.012951\n",
            "Loss at step 17130: 0.023203\n",
            "Loss at step 17140: 0.024413\n",
            "Loss at step 17150: 0.031080\n",
            "Loss at step 17160: 0.046094\n",
            "Loss at step 17170: 0.036205\n",
            "Loss at step 17180: 0.027502\n",
            "Loss at step 17190: 0.039578\n",
            "Loss at step 17200: 0.039093\n",
            "Loss at step 17210: 0.025099\n",
            "Loss at step 17220: 0.045008\n",
            "Loss at step 17230: 0.018831\n",
            "Loss at step 17240: 0.021408\n",
            "Loss at step 17250: 0.033169\n",
            "Loss at step 17260: 0.014795\n",
            "Loss at step 17270: 0.065279\n",
            "Loss at step 17280: 0.042257\n",
            "Loss at step 17290: 0.011715\n",
            "Loss at step 17300: 0.034995\n",
            "Loss at step 17310: 0.014865\n",
            "Loss at step 17320: 0.035690\n",
            "Loss at step 17330: 0.009223\n",
            "Loss at step 17340: 0.042253\n",
            "Loss at step 17350: 0.010441\n",
            "Loss at step 17360: 0.019609\n",
            "Loss at step 17370: 0.030907\n",
            "Loss at step 17380: 0.013430\n",
            "Loss at step 17390: 0.011960\n",
            "Loss at step 17400: 0.037022\n",
            "Loss at step 17410: 0.033699\n",
            "Loss at step 17420: 0.030453\n",
            "Loss at step 17430: 0.035455\n",
            "Loss at step 17440: 0.029565\n",
            "Loss at step 17450: 0.022850\n",
            "Loss at step 17460: 0.047885\n",
            "Loss at step 17470: 0.022252\n",
            "Loss at step 17480: 0.017588\n",
            "Loss at step 17490: 0.045087\n",
            "Loss at step 17500: 0.038213\n",
            "Loss at step 17510: 0.042609\n",
            "Loss at step 17520: 0.034737\n",
            "Loss at step 17530: 0.007930\n",
            "Loss at step 17540: 0.008899\n",
            "Loss at step 17550: 0.048697\n",
            "Loss at step 17560: 0.024262\n",
            "Loss at step 17570: 0.034531\n",
            "Loss at step 17580: 0.034428\n",
            "Loss at step 17590: 0.025928\n",
            "Loss at step 17600: 0.039682\n",
            "Loss at step 17610: 0.031416\n",
            "Loss at step 17620: 0.059707\n",
            "Loss at step 17630: 0.040932\n",
            "Loss at step 17640: 0.015750\n",
            "Loss at step 17650: 0.041136\n",
            "Loss at step 17660: 0.013259\n",
            "Loss at step 17670: 0.027035\n",
            "Loss at step 17680: 0.013862\n",
            "Loss at step 17690: 0.044231\n",
            "Loss at step 17700: 0.038821\n",
            "Loss at step 17710: 0.029598\n",
            "Loss at step 17720: 0.017696\n",
            "Loss at step 17730: 0.027029\n",
            "Loss at step 17740: 0.018137\n",
            "Loss at step 17750: 0.039673\n",
            "Loss at step 17760: 0.023879\n",
            "Loss at step 17770: 0.047429\n",
            "Loss at step 17780: 0.019637\n",
            "Loss at step 17790: 0.012584\n",
            "Loss at step 17800: 0.023701\n",
            "Loss at step 17810: 0.036985\n",
            "Loss at step 17820: 0.008700\n",
            "Loss at step 17830: 0.015896\n",
            "Loss at step 17840: 0.026030\n",
            "Loss at step 17850: 0.007122\n",
            "Loss at step 17860: 0.053947\n",
            "Loss at step 17870: 0.037118\n",
            "Loss at step 17880: 0.021408\n",
            "Loss at step 17890: 0.021574\n",
            "Loss at step 17900: 0.013736\n",
            "Loss at step 17910: 0.055084\n",
            "Loss at step 17920: 0.024100\n",
            "Loss at step 17930: 0.081289\n",
            "Loss at step 17940: 0.013599\n",
            "Loss at step 17950: 0.025477\n",
            "Loss at step 17960: 0.014133\n",
            "Loss at step 17970: 0.018969\n",
            "Loss at step 17980: 0.017281\n",
            "Loss at step 17990: 0.021882\n",
            "Loss at step 18000: 0.023893\n",
            "Loss at step 18010: 0.039090\n",
            "Loss at step 18020: 0.040416\n",
            "Loss at step 18030: 0.034405\n",
            "Loss at step 18040: 0.015732\n",
            "Loss at step 18050: 0.070820\n",
            "Loss at step 18060: 0.030425\n",
            "Loss at step 18070: 0.079300\n",
            "Loss at step 18080: 0.070894\n",
            "Loss at step 18090: 0.040542\n",
            "Loss at step 18100: 0.054878\n",
            "Loss at step 18110: 0.015144\n",
            "Loss at step 18120: 0.028822\n",
            "Loss at step 18130: 0.042608\n",
            "Loss at step 18140: 0.018512\n",
            "Loss at step 18150: 0.040034\n",
            "Loss at step 18160: 0.016282\n",
            "Loss at step 18170: 0.024106\n",
            "Loss at step 18180: 0.021891\n",
            "Loss at step 18190: 0.031888\n",
            "Loss at step 18200: 0.016554\n",
            "Loss at step 18210: 0.031856\n",
            "Loss at step 18220: 0.015425\n",
            "Loss at step 18230: 0.012639\n",
            "Loss at step 18240: 0.023793\n",
            "Loss at step 18250: 0.044059\n",
            "Loss at step 18260: 0.025767\n",
            "Loss at step 18270: 0.021766\n",
            "Loss at step 18280: 0.010743\n",
            "Loss at step 18290: 0.048122\n",
            "Loss at step 18300: 0.090322\n",
            "Loss at step 18310: 0.019646\n",
            "Loss at step 18320: 0.047769\n",
            "Loss at step 18330: 0.021901\n",
            "Loss at step 18340: 0.029748\n",
            "Loss at step 18350: 0.036532\n",
            "Loss at step 18360: 0.032939\n",
            "Loss at step 18370: 0.010534\n",
            "Loss at step 18380: 0.020305\n",
            "Loss at step 18390: 0.030122\n",
            "Loss at step 18400: 0.031839\n",
            "Loss at step 18410: 0.036144\n",
            "Loss at step 18420: 0.020200\n",
            "Loss at step 18430: 0.018747\n",
            "Loss at step 18440: 0.088551\n",
            "Loss at step 18450: 0.053482\n",
            "Loss at step 18460: 0.024741\n",
            "Loss at step 18470: 0.013891\n",
            "Loss at step 18480: 0.011381\n",
            "Loss at step 18490: 0.024772\n",
            "Loss at step 18500: 0.038137\n",
            "Loss at step 18510: 0.020097\n",
            "Loss at step 18520: 0.022187\n",
            "Loss at step 18530: 0.030530\n",
            "Loss at step 18540: 0.030535\n",
            "Loss at step 18550: 0.033743\n",
            "Loss at step 18560: 0.038853\n",
            "Loss at step 18570: 0.010218\n",
            "Loss at step 18580: 0.037960\n",
            "Loss at step 18590: 0.024975\n",
            "Loss at step 18600: 0.021375\n",
            "Loss at step 18610: 0.051227\n",
            "Loss at step 18620: 0.027596\n",
            "Loss at step 18630: 0.018660\n",
            "Loss at step 18640: 0.068790\n",
            "Loss at step 18650: 0.020315\n",
            "Loss at step 18660: 0.025226\n",
            "Loss at step 18670: 0.024085\n",
            "Loss at step 18680: 0.053798\n",
            "Loss at step 18690: 0.022232\n",
            "Loss at step 18700: 0.043248\n",
            "Loss at step 18710: 0.035808\n",
            "Loss at step 18720: 0.045363\n",
            "Loss at step 18730: 0.011042\n",
            "Loss at step 18740: 0.062298\n",
            "Loss at step 18750: 0.028208\n",
            "Loss at step 18760: 0.005582\n",
            "Loss at step 18770: 0.047606\n",
            "Loss at step 18780: 0.058941\n",
            "Loss at step 18790: 0.013554\n",
            "Loss at step 18800: 0.027604\n",
            "Loss at step 18810: 0.028311\n",
            "Loss at step 18820: 0.070114\n",
            "Loss at step 18830: 0.012513\n",
            "Loss at step 18840: 0.016224\n",
            "Loss at step 18850: 0.027356\n",
            "Loss at step 18860: 0.018756\n",
            "Loss at step 18870: 0.027870\n",
            "Loss at step 18880: 0.048490\n",
            "Loss at step 18890: 0.128630\n",
            "Loss at step 18900: 0.025186\n",
            "Loss at step 18910: 0.030314\n",
            "Loss at step 18920: 0.049796\n",
            "Loss at step 18930: 0.020660\n",
            "Loss at step 18940: 0.073076\n",
            "Loss at step 18950: 0.026619\n",
            "Loss at step 18960: 0.040765\n",
            "Loss at step 18970: 0.022197\n",
            "Loss at step 18980: 0.015238\n",
            "Loss at step 18990: 0.061275\n",
            "Loss at step 19000: 0.035687\n",
            "Loss at step 19010: 0.031930\n",
            "Loss at step 19020: 0.012312\n",
            "Loss at step 19030: 0.040760\n",
            "Loss at step 19040: 0.017702\n",
            "Loss at step 19050: 0.019639\n",
            "Loss at step 19060: 0.023133\n",
            "Loss at step 19070: 0.029492\n",
            "Loss at step 19080: 0.033210\n",
            "Loss at step 19090: 0.051468\n",
            "Loss at step 19100: 0.018054\n",
            "Loss at step 19110: 0.044076\n",
            "Loss at step 19120: 0.022229\n",
            "Loss at step 19130: 0.034537\n",
            "Loss at step 19140: 0.020658\n",
            "Loss at step 19150: 0.021290\n",
            "Loss at step 19160: 0.049532\n",
            "Loss at step 19170: 0.053666\n",
            "Loss at step 19180: 0.024334\n",
            "Loss at step 19190: 0.053361\n",
            "Loss at step 19200: 0.066979\n",
            "Loss at step 19210: 0.009150\n",
            "Loss at step 19220: 0.016367\n",
            "Loss at step 19230: 0.011976\n",
            "Loss at step 19240: 0.014402\n",
            "Loss at step 19250: 0.054724\n",
            "Loss at step 19260: 0.020438\n",
            "Loss at step 19270: 0.028575\n",
            "Loss at step 19280: 0.013549\n",
            "Loss at step 19290: 0.023767\n",
            "Loss at step 19300: 0.036209\n",
            "Loss at step 19310: 0.057959\n",
            "Loss at step 19320: 0.040003\n",
            "Loss at step 19330: 0.020698\n",
            "Loss at step 19340: 0.017927\n",
            "Loss at step 19350: 0.023961\n",
            "Loss at step 19360: 0.020289\n",
            "Loss at step 19370: 0.020215\n",
            "Loss at step 19380: 0.047453\n",
            "Loss at step 19390: 0.054081\n",
            "Loss at step 19400: 0.016277\n",
            "Loss at step 19410: 0.050736\n",
            "Loss at step 19420: 0.030413\n",
            "Loss at step 19430: 0.024138\n",
            "Loss at step 19440: 0.028160\n",
            "Loss at step 19450: 0.030632\n",
            "Loss at step 19460: 0.028175\n",
            "Loss at step 19470: 0.017150\n",
            "Loss at step 19480: 0.041706\n",
            "Loss at step 19490: 0.018043\n",
            "Loss at step 19500: 0.059779\n",
            "Loss at step 19510: 0.034530\n",
            "Loss at step 19520: 0.032810\n",
            "Loss at step 19530: 0.051846\n",
            "Loss at step 19540: 0.035221\n",
            "Loss at step 19550: 0.031614\n",
            "Loss at step 19560: 0.027972\n",
            "Loss at step 19570: 0.024201\n",
            "Loss at step 19580: 0.014047\n",
            "Loss at step 19590: 0.043700\n",
            "Loss at step 19600: 0.017773\n",
            "Loss at step 19610: 0.053437\n",
            "Loss at step 19620: 0.034451\n",
            "Loss at step 19630: 0.037744\n",
            "Loss at step 19640: 0.025729\n",
            "Loss at step 19650: 0.036152\n",
            "Loss at step 19660: 0.032953\n",
            "Loss at step 19670: 0.023433\n",
            "Loss at step 19680: 0.025266\n",
            "Loss at step 19690: 0.021862\n",
            "Loss at step 19700: 0.015811\n",
            "Loss at step 19710: 0.021050\n",
            "Loss at step 19720: 0.007414\n",
            "Loss at step 19730: 0.062387\n",
            "Loss at step 19740: 0.018933\n",
            "Loss at step 19750: 0.062992\n",
            "Loss at step 19760: 0.038435\n",
            "Loss at step 19770: 0.020577\n",
            "Loss at step 19780: 0.022260\n",
            "Loss at step 19790: 0.026018\n",
            "Loss at step 19800: 0.041806\n",
            "Loss at step 19810: 0.027564\n",
            "Loss at step 19820: 0.236558\n",
            "Loss at step 19830: 0.022349\n",
            "Loss at step 19840: 0.068949\n",
            "Loss at step 19850: 0.022383\n",
            "Loss at step 19860: 0.015605\n",
            "Loss at step 19870: 0.018165\n",
            "Loss at step 19880: 0.052672\n",
            "Loss at step 19890: 0.040272\n",
            "Loss at step 19900: 0.075850\n",
            "Loss at step 19910: 0.010932\n",
            "Loss at step 19920: 0.043323\n",
            "Loss at step 19930: 0.031281\n",
            "Loss at step 19940: 0.009996\n",
            "Loss at step 19950: 0.028927\n",
            "Loss at step 19960: 0.015442\n",
            "Loss at step 19970: 0.041932\n",
            "Loss at step 19980: 0.023023\n",
            "Loss at step 19990: 0.047466\n",
            "Loss at step 20000: 0.021673\n",
            "Loss at step 20010: 0.014543\n",
            "Loss at step 20020: 0.011267\n",
            "Loss at step 20030: 0.011643\n",
            "Loss at step 20040: 0.018377\n",
            "Loss at step 20050: 0.013765\n",
            "Loss at step 20060: 0.022557\n",
            "Loss at step 20070: 0.018175\n",
            "Loss at step 20080: 0.102227\n",
            "Loss at step 20090: 0.070303\n",
            "Loss at step 20100: 0.060631\n",
            "Loss at step 20110: 0.030595\n",
            "Loss at step 20120: 0.024051\n",
            "Loss at step 20130: 0.039798\n",
            "Loss at step 20140: 0.031123\n",
            "Loss at step 20150: 0.040833\n",
            "Loss at step 20160: 0.059793\n",
            "Loss at step 20170: 0.013079\n",
            "Loss at step 20180: 0.018963\n",
            "Loss at step 20190: 0.036982\n",
            "Loss at step 20200: 0.033381\n",
            "Loss at step 20210: 0.088976\n",
            "Loss at step 20220: 0.017270\n",
            "Loss at step 20230: 0.016848\n",
            "Loss at step 20240: 0.012668\n",
            "Loss at step 20250: 0.022744\n",
            "Loss at step 20260: 0.022486\n",
            "Loss at step 20270: 0.009295\n",
            "Loss at step 20280: 0.012254\n",
            "Loss at step 20290: 0.023282\n",
            "Loss at step 20300: 0.022456\n",
            "Loss at step 20310: 0.044507\n",
            "Loss at step 20320: 0.016795\n",
            "Loss at step 20330: 0.035768\n",
            "Loss at step 20340: 0.018999\n",
            "Loss at step 20350: 0.042379\n",
            "Loss at step 20360: 0.041585\n",
            "Loss at step 20370: 0.013146\n",
            "Loss at step 20380: 0.054617\n",
            "Loss at step 20390: 0.020381\n",
            "Loss at step 20400: 0.023014\n",
            "Loss at step 20410: 0.034203\n",
            "Loss at step 20420: 0.025360\n",
            "Loss at step 20430: 0.039656\n",
            "Loss at step 20440: 0.008025\n",
            "Loss at step 20450: 0.022808\n",
            "Loss at step 20460: 0.017805\n",
            "Loss at step 20470: 0.003980\n",
            "Loss at step 20480: 0.037911\n",
            "Loss at step 20490: 0.040993\n",
            "Loss at step 20500: 0.035442\n",
            "Loss at step 20510: 0.016407\n",
            "Loss at step 20520: 0.026291\n",
            "Loss at step 20530: 0.038795\n",
            "Loss at step 20540: 0.029634\n",
            "Loss at step 20550: 0.044473\n",
            "Loss at step 20560: 0.047326\n",
            "Loss at step 20570: 0.030650\n",
            "Loss at step 20580: 0.028182\n",
            "Loss at step 20590: 0.026471\n",
            "Loss at step 20600: 0.034556\n",
            "Loss at step 20610: 0.049361\n",
            "Loss at step 20620: 0.078894\n",
            "Loss at step 20630: 0.021632\n",
            "Loss at step 20640: 0.028203\n",
            "Loss at step 20650: 0.037925\n",
            "Loss at step 20660: 0.014368\n",
            "Loss at step 20670: 0.091122\n",
            "Loss at step 20680: 0.033636\n",
            "Loss at step 20690: 0.052243\n",
            "Loss at step 20700: 0.053304\n",
            "Loss at step 20710: 0.053614\n",
            "Loss at step 20720: 0.037451\n",
            "Loss at step 20730: 0.074765\n",
            "Loss at step 20740: 0.016921\n",
            "Loss at step 20750: 0.028539\n",
            "Loss at step 20760: 0.024027\n",
            "Loss at step 20770: 0.060797\n",
            "Loss at step 20780: 0.025135\n",
            "Loss at step 20790: 0.048782\n",
            "Loss at step 20800: 0.020480\n",
            "Loss at step 20810: 0.020780\n",
            "Loss at step 20820: 0.066962\n",
            "Wed Feb  5 14:35:38 UTC 2020\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
